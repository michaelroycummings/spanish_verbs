{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish Verbs in Embedding Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose\n",
    "- practice matrix math with embedding space\n",
    "- practice creating language model\n",
    "- practice using pretrained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "\n",
    "<u>Spanish Corpora and Word Lists</u>\n",
    "\n",
    "| Type       | Source        | Link |\n",
    "|------------|---------------|------|\n",
    "| Corpus     | Kaggle        | [120M Words](https://www.kaggle.com/datasets/rtatman/120-million-word-spanish-corpus) |\n",
    "| Corpus     | HuggingFace   | [LargeSpanishCorpus](https://huggingface.co/datasets/large_spanish_corpus) |\n",
    "| Corpus     | HuggingFace   | [SpanishBillionWords](https://huggingface.co/datasets/spanish_billion_words) |\n",
    "| Words      | Github        | [lorenbrichter](https://raw.githubusercontent.com/lorenbrichter/Words/master/Words/es.txt) |\n",
    "| Verbs      | Github        | [bretttolbert](https://github.com/bretttolbert/verbecc/blob/main/verbecc/data/verbs-es.xml) |\n",
    "| Verbs      | Github        | [ghidinelli](https://github.com/ghidinelli/fred-jehle-spanish-verbs/blob/master/jehle_verb_database.csv) |\n",
    "\n",
    "\n",
    "\n",
    "<u>Parts of Speech Checkers</u>\n",
    "- Hard coded: [SpanishDict](https://api.spanishdict.com/api/v1/wordoftheday/{word}')\n",
    "- ML: some language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan\n",
    "1. Get words counts from hugging face.\n",
    "2. verbs list from spanish dict hardcoded method\n",
    "3. verbs list from some ml model\n",
    "4. Comparisons of the two:\n",
    "    - FILL\n",
    "5. Create baseline distance:\n",
    "    - Min and max distance between verbs\n",
    "    - Standard deviation of distance between verbs\n",
    "    - Average distance of kth closest verb for first 30 k's\n",
    "        - For every verb, calculate distance of 30 closest verbs, then average over all verbs\n",
    "    - Sanity check: Average distance between a few verbs and their similarity verb clusters\n",
    "6. Create tuples of re verbs and their non re counterparts\n",
    "7. Tests on RE verbs:\n",
    "    - 2D feature reduction:\n",
    "        - plot tuples in 2D space\n",
    "    - Embedding distance between tuples\n",
    "        - Min, max, mean, std dev\n",
    "        - plot distribution\n",
    "        - plot distance vs length of verb\n",
    "        - plot distance vs use of verbs (which metric to combine uses? average or max or both?)\n",
    "    - Kth closest verb for each item of each tuple\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import random\n",
    "\n",
    "def get_verbs_from_bred() -> list:\n",
    "    \"\"\"\n",
    "    Retrieves a list of verbs and definitions from  bretttolbert's verbecc project.\n",
    "\n",
    "    Returns:\n",
    "    - A list of verbs.\n",
    "    \"\"\"\n",
    "    # Send a GET request to fetch the XML content\n",
    "    url = \"https://github.com/ghidinelli/fred-jehle-spanish-verbs/blob/master/jehle_verb_database.csv\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the XML content\n",
    "    xml_content = response.content\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    # Find all verbs\n",
    "    verb_elements = root.findall(\".//v\")\n",
    "    verbs = [v.find(\"i\").text for v in verb_elements]\n",
    "    return verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "def get_verbs_from_fred() -> list:\n",
    "    \"\"\"\n",
    "    Retrieves a list of verbs from ghidinelli's fred-jehle-spanish-verbs project.\n",
    "\n",
    "    Returns:\n",
    "    - A list of verbs.\n",
    "    \"\"\"\n",
    "    verbs = {}\n",
    "    url = 'https://raw.githubusercontent.com/ghidinelli/fred-jehle-spanish-verbs/master/jehle_verb_database.csv'\n",
    "    response = requests.get(url)\n",
    "    file = StringIO(response.text)\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        verb = row[0]\n",
    "        definition = row[1]\n",
    "        if verb not in verbs:\n",
    "            verbs[verb] = definition\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_corpus_as_bow() -> list:\n",
    "    \"\"\"\n",
    "    Parses the HuggingFace large_spanish_corpus dataset into bag of words.\n",
    "    Uses a single function for both to avoid having to save the entire\n",
    "    dataset to disk.\n",
    "\n",
    "    Returns:\n",
    "    - A bag of words.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        'large_spanish_corpus', name='EUBookShop', split='train',\n",
    "        streaming=True, trust_remote_code=True)\n",
    "\n",
    "    # Default Dictionary will output 0 for queried keys that don't exist\n",
    "    bow = defaultdict(int)\n",
    "\n",
    "    # Iterate over each batch of texts in the dataset\n",
    "    for batch in dataset:\n",
    "        # Tokenize the text into individual words\n",
    "        words = batch['text'].split()\n",
    "        # Iterate over each word\n",
    "        for word in words:\n",
    "            bow[word] += 1\n",
    "\n",
    "    sorted_bow = sorted(list(bow.items()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save the dictionary\n",
    "    with open('sorted_bow.pkl', 'wb') as f:\n",
    "        pickle.dump(sorted_bow, f)\n",
    "\n",
    "    return sorted_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bow() -> defaultdict:\n",
    "    \"\"\"\n",
    "    Loads the sorted bag of words from disk.\n",
    "\n",
    "    Returns:\n",
    "    - A BOW as a defaultdict(int) object.\n",
    "    \"\"\"\n",
    "    with open('sorted_bow.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\micha\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import pipeline\n",
    "\n",
    "def verbs_from_words(bow: list) -> list:\n",
    "    \"\"\"\n",
    "    Uses HuggingFace's bert-base-multilingual-cased LLM for\n",
    "    parts-of-speech tagging to extract a bag of verbs from\n",
    "    a bag of words.\n",
    "\n",
    "    Returns:\n",
    "    - A bag of verbs.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = pipeline('ner', model='bert-base-multilingual-cased')\n",
    "\n",
    "    bov = []\n",
    "\n",
    "    # Find verbs in the bow\n",
    "    for word, count in bow:\n",
    "        pos = nlp(word)\n",
    "        for token in pos:\n",
    "            bov.append([word, count])\n",
    "\n",
    "    return bov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endswith(bow: list) -> list:\n",
    "    \"\"\"\n",
    "    Only words ending in 'ar', 'er', 'ir'.\n",
    "    \"\"\"\n",
    "    bov = []\n",
    "    for word, count in bow:\n",
    "        if word.endswith(('ar', 'er', 'ir')):\n",
    "            bov.append([word, count])\n",
    "    return bov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def diff_bov_bred(bov: defaultdict(int), bred: list) -> defaultdict(int):\n",
    "    \"\"\"\n",
    "    Returns the difference between the bag of verbs and the list of verbs\n",
    "    from bretttolbert's verbecc project.\n",
    "\n",
    "    Returns:\n",
    "    - A bag of verbs as a defaultdict(int) object.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'bov not bret': {k: bov[k] for k in set(bov) - set(bred)},\n",
    "        'bret not bov': {k: bred[k] for k in set(bred) - set(bov)}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 625/625 [00:00<00:00, 627kB/s]\n",
      "model.safetensors: 100%|██████████| 714M/714M [02:42<00:00, 4.40MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\micha\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 29.2kB/s]\n",
      "vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 3.51MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 3.71MB/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bow \u001b[38;5;241m=\u001b[39m load_bow()\n\u001b[1;32m----> 2\u001b[0m bov1 \u001b[38;5;241m=\u001b[39m \u001b[43mverbs_from_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m bov2 \u001b[38;5;241m=\u001b[39m endswith(bow)\n",
      "Cell \u001b[1;32mIn [6], line 19\u001b[0m, in \u001b[0;36mverbs_from_words\u001b[1;34m(bow)\u001b[0m\n\u001b[0;32m     16\u001b[0m bov \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Find verbs in the bow\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m():\n\u001b[0;32m     20\u001b[0m     pos \u001b[38;5;241m=\u001b[39m nlp(word)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m pos:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "bow = load_bow()\n",
    "bov1 = verbs_from_words(bow)\n",
    "bov2 = endswith(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(x\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(word)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "x = defaultdict(int)\n",
    "x['2'] = 10\n",
    "x['3'] = 11\n",
    "y = sorted(x.items(), key=lambda x: x[1], reverse=True)\n",
    "for word in y.keys():\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
