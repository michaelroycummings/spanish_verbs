{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish Verbs in Embedding Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This section serves as the manager of the project, and will cover the following topics:\n",
    "- **Aim and Goals**: clarify the focus of the project.\n",
    "- **Plan**: clearly state each step of the project.\n",
    "    - Although this seems a little self explanatory, exploratory data analysis (EDA) heavy projects like this one can easily rabbit-hole. The plan will be iterated on continuously and serve to determine whether a question that arises from EDA helps the final goal, before that question is explored and possibly branched further into new questions. \n",
    "- **Resources**: serves as a link lookup for each data needed, and allows us to verify whether we can access all the data necessary for our current plan before getting halfway and realizing data constraints mean we need to change the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim and Goals\n",
    "This project has a very simple idea: in English, verbs that start with \"re\", that also have a non-re counterpart (e.g., redo and do) mean \"to do the verb again\".\n",
    "As I've been learning Spanish over the past few months, I've noticed verbs that do and do not follow this rule, so the project aims to use the embedding space of pre-trained language models to explore the adherence of Spanish verbs to this rule.\n",
    "\n",
    "Although the aim is simple, this is my first project after a year of self studying my Master's in Data Science, so this project will be overly verbose, branching into side ideas to practice and iron out the application kinks in as many topics as I can.\n",
    "\n",
    "An adaptive list:\n",
    "- practice matrix math with embedding space\n",
    "- practice creating language model\n",
    "- practice using pretrained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "1. We need a Parts-of-Speech model to label a Spanish corpus as verbs or not.\n",
    "    1. Will need to test POS model's ability before using it to get a large list of verbs.\n",
    "        1. Find premade list of verbs.\n",
    "        2. Test POS performance on premade verb list.\n",
    "2. Get Spanish Verbs\n",
    "    1. Find Spanish Corpus\n",
    "    2. Use POS to filter for Verbs.\n",
    "    3. Create Bag-of-Words\n",
    "3. Get embedding models.\n",
    "    - Explain how transformers create embeddings from text using Hugging Face and BERT\n",
    "    - Hugging Face Transformer\n",
    "    - SpaCy tok2vec component\n",
    "    - Fasttext: will be baseline\n",
    "    - Maybe use glove or word2vec as other baselines\n",
    "4. Create baseline distance:\n",
    "    - Min and max distance between verbs\n",
    "    - Standard deviation of distance between verbs\n",
    "    - Average distance of kth closest verb for first 30 k's\n",
    "        - For every verb, calculate distance of 30 closest verbs, then average over all verbs\n",
    "    - Sanity check: Average distance between a few verbs and their similarity verb clusters\n",
    "5. Create tuples of re verbs and their non re counterparts\n",
    "6. Tests on RE verbs:\n",
    "    - 2D feature reduction:\n",
    "        - plot tuples in 2D space\n",
    "    - Embedding distance between tuples\n",
    "        - Min, max, mean, std dev\n",
    "        - plot distribution\n",
    "        - plot distance vs length of verb\n",
    "        - plot distance vs use of verbs (which metric to combine uses? average or max or both?)\n",
    "    - Kth closest verb for each item of each tuple\n",
    "    \n",
    "\n",
    "Other: should I add sense2vec or other sense vector models and compare the words, somehow, in each sense? At least mention sense vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "<u>Spanish Corpora and Word Lists</u>\n",
    "\n",
    "| Type       | Source        | Link |\n",
    "|------------|---------------|------|\n",
    "| Corpus     | Kaggle        | [120M Words](https://www.kaggle.com/datasets/rtatman/120-million-word-spanish-corpus) |\n",
    "| Corpus     | HuggingFace   | [LargeSpanishCorpus](https://huggingface.co/datasets/large_spanish_corpus) |\n",
    "| Corpus     | HuggingFace   | [SpanishBillionWords](https://huggingface.co/datasets/spanish_billion_words) |\n",
    "| Words      | Github        | [lorenbrichter](https://raw.githubusercontent.com/lorenbrichter/Words/master/Words/es.txt) |\n",
    "| Verbs      | Github        | [bretttolbert](https://github.com/bretttolbert/verbecc/blob/main/verbecc/data/verbs-es.xml) |\n",
    "| Verbs      | Github        | [ghidinelli](https://github.com/ghidinelli/fred-jehle-spanish-verbs/blob/master/jehle_verb_database.csv) |\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Parts of Speech Checkers</u>\n",
    "- Hard coded: ar, er, ir\n",
    "- HuggingFace: [Spanish RoBERTa](https://huggingface.co/PlanTL-GOB-ES/roberta-large-bne-capitel-pos)\n",
    "- Spacy: [Spanish pipelines](https://spacy.io/models/es#es_core_news_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things Learnt\n",
    "1. Transformer's have contextual embeddings (think multi-head attention), so they cannot provide an embedding for stand-alone words.\n",
    "2. some verbs have .pos_ 'AUX' not 'VERB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things I Would Do With More Time\n",
    "1. Try/except + logging clauses for code that downloads the corpus data and derives the word embeddings.\n",
    "    - On each instance in case some instances are not cleaned or have quirks\n",
    "    - On each REST request, as these do not always execute perfectly (e.g., signal drops, rate limit hit, etc.)\n",
    "    * This would add a step during analysis to ensure models have similar sample sizes of embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology and Background Info\n",
    "As many topics in natural language processing (NLP) are new and evolving, terminology is sometimes used to mean multiple things, which is detrimental to non-experts (like me) who do not have the context to assume the true meaning in each case. We will set clear definitions for these terms to avoid confusion throughout the project.\n",
    "\n",
    "As well, we will briefly explain model concepts so that decisions and comparisons made throughout the project can be appreciated by non-experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarifying Common Terms\n",
    "Below is a clarification of some terminology that I've often seen used a little less specifically than is useful for people entering the space.\n",
    "\n",
    "Processes:\n",
    "- **Encoding:**\n",
    "  - **Meaning:** Tokenization and then numericalization. Takes text data, tokenizes it into string or byte tokens, and outputs a numerical scalar, called a token ID, for each token in the input sequence.\n",
    "  - **Conflated with:** Sometimes used to mean just tokenization.\n",
    "- **Tokenization:**\n",
    "  - **Meaning:** Splitting text data into smaller text data or into byte-data based on some rules.\n",
    "  - **Conflated with:** Used with about a 50 / 50 to mean solely tokenization, or tokenization and numericalization.\n",
    "- **Numericalization:**\n",
    "  - **Meaning:** Mapping a token to a numerical scalar called a token ID.\n",
    "  - **Conflated with:** This is almost never mentioned, but its crucial to understanding how the feature space of ML embedding models begin.\n",
    "- **Embedding Lookup:**\n",
    "  - **Meaning:** Mapping a token ID to position in multidimensional feature space, which we will call **input token embeddings**. These non-learnable embeddings are the tokens of the tokenizer, in the language of the embedding model. They are non-learnable, stored in an embedding matrix, and made based on a single vocabulary, to be sufficiently separated so that the embedding model can learn how to associate the tokens without a preconceived bias in the form of some tokens being closer together than others.\n",
    "  - **Conflated with:** This is also almost never mentioned, but it is so crucial! To understanding how the feature space of ML embedding models begin.\n",
    "    - **Note:** The term token embedding or word embedding is conflated constantly: when talking about the parts of a model, input word embeddings are simply called word embeddings. But when talking about uses of embedding models, word embeddings are the output of the embedding model for each token. So we will continue using input and output to distinguish the two. \n",
    "- **Embedding:**\n",
    "  - **Meaning:** Mapping a numerical vector that lacks semantic or contextual meaning (an input token embedding) to a numerical vector with semantic or contextual meaning (an output token embedding).\n",
    "  - **Conflated with:** Sometimes used to mean tokenization.\n",
    "\n",
    "Algorithms:\n",
    "- **Tokenizer:**\n",
    "  - **Meaning:** When the tokenizer is stand-alone and is not used for a specific model, it means an algorithm that solely performs tokenization. When the tokenizer is meant to be used for a specific, pre-trained model, it usually means an algorithm that includes the tokenization and numericalization steps, as the word embedding model almost never takes tokens in the form of strings or bytes.\n",
    "  - **Conflated with:** Sometimes used to mean an algorithm that performs the whole pipeline of tokenization, numericalization, embedding lookup, and embedding.\n",
    "- **Embedding model**:\n",
    "- **Language model**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder vs Decoder\n",
    "\n",
    "The distinction between encoder and decoder has different meanings even inside ML. In terms of **autoencoders**, an encoder learns a mapping from input feature space to a lower dimensional feature space, and a decoder learns to reconstruct the original feature space from the lower dimensional space.\n",
    "\n",
    "This is the understanding of encoders and decoders that I see most often, and its proliferation is shown in AI chatbots. When I asked Bing copilot to explain the difference between encoder and decoders, and have it align with GPT being decoder only and BERT being encoder only, it was unable to deviate from this common explanation.\n",
    "\n",
    "In the context of sequential data however:\n",
    "- **Encoders** are bidirectional and non auto-regressive. I.e., models that have access to the entire sequence before yielding an output that will be used.\n",
    "- **Decoders** are unidirectional and auto-regressive. I.e., models that have access to only previous elements of a sequence before yielding an output that will be used.\n",
    "    - During training, auto-regressive models use **teacher forcing**, which means using the next element of the ground-truth labels as input instead of the model's last output, to avoid compounding errors. Giving future elements to the auto-regressive model would \"cheat\", i.e., would train the model in an environment that will not exist during application.\n",
    "    - During prediction, future elements of the sequence do not exist, so a decoder cannot use the whole sequence.\n",
    "\n",
    "Here are some examples to solidify the definitions:\n",
    "- **Encoder-decoder RNN model**: this model type is often used for neural machine translation (NMT), i.e., using neural networks to translate between languages. The encoder RNN takes one element of the sequence per timestep, but its output (specifically, the hidden state of its highest layer) is not used until the encoder RNN sees the entire input sequence. The decoder RNN also takes one element of the sequence per time step, and its output is used as every timestep, while taking the output of the encoder RNN, that has used the entire encoder input sequence, at every timestep.\n",
    "- **A Seq2Seq RNN decoder**: to derive a single value from a sequence, you may use an Seq2Seq RNN instead of a Seq2Vec to give more comparisons to the loss function. In this case, the model is producing an output for every (or at least some) of the elements of the input sequence, but only the final output value will be used, and the model will have seen the entire input sequence before yielding this value, so this is the design of an encoder.\n",
    "- **GPT**: a unidirectional, auto-regressive transformer model that uses masked self-attention. In the [original paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) it used next word prediction for pre-training.\n",
    "- **BERT**: BERT and [its variants](https://www.tensorflow.org/text/tutorials/classify_text_with_bert#loading_models_from_tensorflow_hub) are bidirectional, non auto-regressive transformer models that create an embedding based on the entire input sequence before an output layer or component uses that embedding for some task.\n",
    "\n",
    "Note that encoder-only models like BERT, and decoder-only models like GPT, can both perform classification and natural language generation tasks:\n",
    "- GPT's auto-regressive architecture naturally lends itself to natural language prediction, but its final output vector, the *extract* token in the [original paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) can be used as an embedding of the entire input sequence for classification tasks.\n",
    "- BERT bidirectional architecture naturally lends itself to classification tasks, but the first element of its output vector, the *CLS* token in the [original paper](https://arxiv.org/pdf/1810.04805.pdf), can be fed to a decoder model for text generation tasks like text summation.\n",
    "\n",
    "Maybe links to add:\n",
    "- lack of congruity on the topic [here](https://datascience.stackexchange.com/questions/118260/chatgpts-architecture-decoder-only-or-encoder-decoder)\n",
    "- T5 paper, section 3.2.1, discussing model structures [here](https://arxiv.org/pdf/1910.10683.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of an ML NLP Model\n",
    "\n",
    "\n",
    "Trainable NLP models can be thought of as three distinct parts:\n",
    "- **Tokenizer model**: a set of pre-defined rules that performs normalization (cleaning raw text), tokenization (dividing cleaned text data it into string or byte tokens) and numericalization (maps each token to numerical scalar called a **token ID**). \n",
    "- **Embedding model**: a embedding matrix that performs embedding lookup (each token ID is mapped to an input embedding token in embedding space) followed by a set of a set of neural layers that performs embedding (maps input token embeddings to output token embeddings in a learnable embedding space that represents features of speech, with respect to that embedding lookup mapping).\n",
    "- **Task head**: one or more neural layers that map the output token embeddings to the output(s) needed for a specific task.\n",
    "\n",
    "Each pre-trained model has a specific tokenizer that preprocessed the data before training. An embedding model can only be used with its own tokenizer because the embedding model understands language in the context of the input token embeddings, and the map from a language to those embeddings are stored in the tokenizer.\n",
    "\n",
    "A single training instance is usually one or more sentences. Each token in that instance is an element of the sequence for that instance.\n",
    "\n",
    "As well as token IDs, some tokenizers will pass position IDs and mask IDs to the embedding model. The embedding model will have a separate embedding lookup matrix for each type of ID. Each matrix will output the same number of dimensions, and the input token embeddings will be a sum of the embedding for each ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Models\n",
    "\n",
    "The title to this section is vague as many models fall into multiple categories: tokenizers, embedding models,  language models.\n",
    "\n",
    "<u>Ignores Semantics and Context</u>\n",
    "- Models that cannot capture semantics nor context are used solely for tokenization.\n",
    "- Examples: bag of words (BOW), term frequency inverse document frequency (TF-IDF).\n",
    "\n",
    "<u>Captures Semantics, Ignores Context</u>\n",
    "- Current models that capture semantics are neural networks comprised solely of simple feed forward layers. They are mainly used to tokenize and embed text before giving those embeddings to a more complex model for further embedding and task output, but can also be used themselves for non natural language generation tasks.\n",
    "- Examples: Word2Vec, GloVe, FastText\n",
    "\n",
    "<u>Captures Semantics and Context</u>\n",
    "- Current models that capture both semantics and context are neural networks comprised of layers that can combine multiple elements of a multidimensional sequence, i.e., recurrent, convolutional, and attention layers (the distinction here being that feed forward layers handle elements of a sequence separately).\n",
    "- They are mainly used to tokenize, embed, and then perform NLP tasks that can involve natural language generation tasks, but can also be used solely for their embeddings.\n",
    "- Examples:\n",
    "    - Encoders like BERT and its variants will usually use a task head of simple feed forward layers for classification tasks, and a complex decoder for natural language generation tasks, giving both task heads as input, the final embedding of a single token, usually a special token that does not correspond to any input text data  (e.g., CLS for BERT).\n",
    "    - Decoders like GPT will usually also use a task head of simple feed forward layers for classification tasks, and its sequence of outputs for natural language generation tasks. For the classification tasks, a task head would also receive the final embedding of a single, special token (e.g., the extract token for GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "Now that we have a good understanding of the dataflow through a transformer model, lets discuss word embeddings.\n",
    "\n",
    "**Word embeddings** are multidimensional vectors that should carry semantic structures of language, as well as contextual structures if the model is contextual. The reasoning is that, if a neural network is trained on one or more tasks that involve understanding speech, the hidden layers are likely capturing structures present in that language.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Extracting Word Embeddings</u>\n",
    "\n",
    "Since different architectures have different methods for mapping hidden layer outputs to the output layer and thus to the loss function, how we derive word embeddings is architecture dependent:\n",
    "- **Word2Vec**: both model types - CBOW and skip-gram - use a single feed forward hidden layer that is unique in the sense that it is connected to every output of every sequence element of the input layer, instead of the canonical dense layer practice of handling sequence elements independently. The output layer is connected to all neurons of the hidden layer, so the hidden layer represents semantic structure of the entire input text. Thus, we will pass single words to this model and use its entire hidden layer as the word embedding for the input word.\n",
    "- **FastText**: their models upgrade the tokenizer to a subword approach but use the same architecture as Word2Vec models, so we will derive their word embeddings in the same way.\n",
    "- **SpaCy models**: connects a word embedding component called `tok2vec` to multiple task-specific components that each use the `tok2vec`'s output as well as the output of necessary task-specific components. Thus, the `tok2vec` component is trained on multiple tasks. SpaCy employs a CNN and transformer `tok2vec` implementation, both of which have all of their token elements used in the task specific layers. This is in comparison to BERT and GPT-like architectures that use a single token for supervised learning tasks. Thus, we will take only the final layer embeddings of these models as the token vectors.\n",
    "- **Transformers**: these models often use the embeddings of a single, special token (e.g. CLS for BERT or \"predict\" for GPT) that does not represent any input data. This token has the possibility of being influenced by any token embedding at any transformer layer, with higher layers having a more likely influence. Thus, common practice for deriving word embeddings from transformers is to sum the embeddings of the last few layers for each token that makes up the target word in the input.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Contextually Dependent or Independent Word Embeddings</u>\n",
    "\n",
    "It could be interesting to compare the embedding space of context-independent and dependent word embedding models, so we will use both types in our analysis.\n",
    "\n",
    "Inputs:\n",
    "- To the context-independent models, we will input sole verbs and document the embedding space for each verb.\n",
    "- To the context-independent models, we will input each sample instance found in the corpus, and a verb embedding will be the average of that word embedding in each sentence containing the word. As well, we will note the word counts incase very infrequent verbs change the results of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText\n",
    "\n",
    "The **FastText model** is the skip-gram model using a [subword vocabulary]((https://arxiv.org/pdf/1607.04606.pdf)) made of all n-grams found in the training corpus, where n is a single choice between 3-6. Each word is represented as the sum of it's n-gram's token embeddings.\n",
    "\n",
    "The [original](https://arxiv.org/pdf/1310.4546.pdf) **skip-gram model** uses a single feed forward hidden layer that is fully connected to each feature of each element in the input sequence. Note this difference from how PyTorch and Tensorflow implement feed forward layers, which is to have it fully connected solely to the last dimension of the input shape, which results in applying the layer independently to each element of a sequence passed to the layer.\n",
    "\n",
    "As an example of the subword tokenizer, the word `happy` for a FastText model of n=4 is represented as the sum of the token embeddings for the tokens `<hap`, `happ`, `appy`, `ppy>`.\n",
    "\n",
    "FastText hosts models [pre-trained](https://fasttext.cc/docs/en/crawl-vectors.html) with the following specs:\n",
    "- Used the Common Crawl and Wikipedia text databases.\n",
    "- Used the CBOW pre-training task with a window of 5 context words and 10 negative samples.\n",
    "- Hidden layer has 300 neurons so each word embedding is 300D.\n",
    "- Tokenizer used a 5-gram subword vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy\n",
    "\n",
    "SpaCy offers two types of ready-made models:\n",
    "- [Pipeline models](https://spacy.io/models#design-cnn): models labelled `sm`/`md`/`lg` are a pipeline of [single-task](https://spacy.io/usage/processing-pipelines) CNN components that inherit from the (1) the tokenizer and (2) the last component output ([visual](https://spacy.io/models#design-cnn)). The exception being the Named Entity Recognition component which has its own trainable tokenizer. Thus, we will use the tokenizer trained on multiple tasks.\n",
    "- [Transformer models](https://spacy.io/models#design-trf): models labelled `trf` are pipeline models that have only two differences from their non-transformer counterparts: (1) the tokenizer is transformer, and (2) the NER component does not have its own tokenizer. Thus, the only choice for non-task specific embedding layers is the transformer tokenizer component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Models from HuggingFace \n",
    "\n",
    "Hugging Face is a machine learning platform that hosts an open-source database of pre-trained transformer models. Each model on the hub has a model card, that explain can explain the model's architecture, pre-training tasks, and usage via Hugging Face's `transformers` python library.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Loading and Usage</u>\n",
    "\n",
    "Many users create model cards that point to the pre-trained model file but do not create functionality for the transformers library. If the model is configured with the library, it can be used out-of-the-box via the [pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) functionality, or handled with more flexibility via an `AutoModel` [class](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel).\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Fine-Tuning</u>\n",
    "\n",
    "Models configured with `transformers` can be fine-tuned using the in-house `Trainer` [class](https://huggingface.co/docs/transformers/v4.37.1/en/main_classes/trainer#transformers.Trainer), or easily converted to work within the [PyTorch](https://pytorch.org/hub/huggingface_pytorch-transformers/) and [TensorFlow](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) python library ecosystems.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Preprocessing</u>\n",
    "\n",
    "Each model is associated with the specific tokenizer that was used for pre-training. \n",
    "\n",
    "The tokenizer outputs scalar IDs, and the transformer model's first layer is an embedding layer that performs embedding lookup to map each ID to an embedding vector. Hugging Face tokenizers output token IDs and attention IDs, as well as token type IDs for some models. The subsequent transformer model will also require position IDs, which it assigns itself, and possible language IDs, which users currently have to prepare manually.\n",
    "\n",
    "Each ID type is described below.\n",
    "\n",
    "| ID Type       | Description |\n",
    "| ------------- | ----------- |\n",
    "| Token IDs     | These map each token string or byte to a scalar integer. |\n",
    "| Token Type IDs| Used in models like BERT to differentiate between sentences in a pair. |\n",
    "| Attention IDs | Used to mask out padding tokens, to create attention windows, or for masked attention in auto-regressive models.|\n",
    "| Position IDs  | Only used for bidirectional / non auto-regressive models.|\n",
    "| Language IDs  | Only used for multilingual or cross-lingual models (e.g., [XLM](https://arxiv.org/pdf/1901.07291.pdf)) and currently, must be manually created.|\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Layers and Outputs - A Deep-ish Dive</u>\n",
    "\n",
    "**BERT** or \"Bidirectional Encoder Representations from Transformers\" is a popular bidirectional / non-masked transformer model, whose [architecture](https://arxiv.org/pdf/1810.04805.pdf) has remained the same in many newer models that improve the original training methods and training tasks used ([RoBERTa](https://arxiv.org/pdf/1907.11692.pdf), [XLM]()) or whose parameter sharing allows for a smaller model ([ALBERT](https://arxiv.org/pdf/1909.11942.pdf)).\n",
    "\n",
    "We will use BERT to understand how to access components, layers, attention matrices, and outputs of a model in the Hugging Face `transformer` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(\n",
    "    model_name, output_hidden_states = True,\n",
    "    output_attentions = True, return_dict = True)\n",
    "\n",
    "example_text = \"El gato está en la caja. El perro lo busca a el.\" # The cat is in the box. The dog looks for him.\n",
    "example_text = \"El gato está en la caja. El perro lo busca a el. Hace dos dias habia mucha gente que bailan por toda la noche y al final, todo se fue feliz. Tambien, el dijo a todo que tuvo un gran tiempo.\" # The cat is in the box. The dog looks for him.\n",
    "token_ids = tokenizer(example_text, return_tensors=\"pt\") # returns PyTorch tensors\n",
    "outputs = model(**token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer used for BERT implements **WordPiece** tokenization, a sub-word tokenization method that starts with a vocabulary of all characters in the training data, and for k steps, merges two tokens in the vocabulary, including merged tokens, to create the next most seen sub-word. BERT's tokenizer then adds a CLS token to the start of each sequence, and a SEP token between each sentence in a single sample instance (relevant for tasks like next sentence prediction and sentence comparison). Then the tokenizer performs padding and truncation if necessary, and then maps each str token to a integer scalar, called a token ID. Finally, it outputs three features for each token: token IDs, position IDs, and attention IDs.\n",
    "\n",
    "Even though we have two sentences, our model does not have a task head that can handle next sentence prediction, so the corresponding tokenizer does not differentiate between the sentences, and gives tokens in both sentences the same attention ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(example_text)\n",
    "print(f\"Number of tokens: {len(inputs['input_ids'])}\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll go through each component of BERT to understand how the sub-word representing token IDs are transformed to a embedding that contains semantic and contextual info, and further transformer to the output of a task.\n",
    "\n",
    "Hugging Face's BERT implementation is divided into 3 components:\n",
    "- \"Embedding\": performs embedding lookup on each ID feature and combines the ID features\n",
    "- \"Encoder\": performs the semantic and contextual embedding\n",
    "- \"Pooler\": takes the embedding at the CLS token position and applies a single feed forward layer.\n",
    "\n",
    "<br>\n",
    "\n",
    "A note on the Pooler and Task Heads:\n",
    "\n",
    "When using a transformer for a specific task, a \"*task head*\" will be attached, which is simply an output layer that matches the outputs needed for the task, and possibly a few trainable layers between the ransformers pre-trained embedding layers and the output layer.\n",
    "\n",
    "The user can choose to feed the pooler's output to the task head, but most implementations I've seen on Hugging Face simply delete it and add their own task head that directly takes the CLS token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT's embedding layer stores three embedding matrices that were trained along with the rest of the model during pre-training. It first performs embedding lookup to transform the three ID features from sequences of scalars to sequences of multidimensional vectors.\n",
    "\n",
    "The sum of the three lookup embedding vectors is then layer normalized for each element of the input sequence to create a single input token embedding for each element the input sequence (see [source code](https://github.com/huggingface/transformers/blob/0b5c7e4838fddeb4601acbb80cd2c71ddcf76726/src/transformers/models/bert/modeling_bert.py#L234) if interested).\n",
    "\n",
    "**Layer Normalization** means normalization applied to each unit of a layer. For attention layers, this means that a mean and variance are calculated for each element in the sequence, across all features of that element. In the context of input token embeddings, each token embedding will be normalized across its embedding dimensions.\n",
    "\n",
    "Note that PyTorch's `LayerNormalization` implementation applies, by default, a learnable affine transformation after normalization, meaning you must set the seed of the random generator to the same value if you want to train the normalization layer multiple times and return the same value for the same dataset.\n",
    "\n",
    "**Dropout** is a regularization technique that involves setting the activations of a given percentage of elements in a layer to zero, resulting in a thinned neural network. The [original paper](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) suggests reselecting the thinned network for every training instance, but [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) implementations both perform it once per batch / step. In the context of input token embeddings, each element of the embedding vector of each sequence has an independent chance of being dropped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT's \"encoder\" component contains all of its 12 transformer layers. Note that a 24 layer version also exists.\n",
    "\n",
    "Each layer performs bidirectional self-attention with 12 heads, and then 3 feed forward layers sandwiched with normalization layers, skip connections, and a GELU activation function. The bottom of [this post](https://gmihaila.medium.com/%EF%B8%8F-bert-inner-workings-1c3054cd1591) has a great diagram of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets go over what outputs we can get from a Hugging Face model:\n",
    "- `attentions`: contain the attention matrices for each head of each self-attention layer of the model. One attention matrix has shape `[sequence_length, sequence_length]` and shows, for each query, the distance between that query and every key, in embedding space. This model has 12 self-attention layers, each with 12 heads, so `12*12` attention matrices.\n",
    "- `hidden_states`: the activations (i.e., final output embeddings) of the embedding layer and the 12 `BertLayer`, so 13 matrices with shape `[sequence_length, model dimensions]`\n",
    "- `last_hidden_state`: the activations of the last attention layer. Note [post processing](https://huggingface.co/docs/transformers/en/main_classes/output#model-outputs) functions may cause this value to be unequal to `hidden_states[-1]`.\n",
    "- `pooler_output`: the activations of the feed forward layer that transformer's the CLS token's final embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Attention matrices: {len(outputs.attentions)} layers of {outputs.attentions[0].shape[1]} \"\n",
    "    + f\"heads. Each matrix has shape {list(outputs.attentions[0].shape[2:])} meaning \"\n",
    "    + \"[sequence length of queries, sequence length of keys]\")\n",
    "print(\n",
    "    f\"Hidden states: {len(outputs.hidden_states)} sets (for the embedding layer + 12 attention \"\n",
    "    + f\"layers) of shape {list(outputs.hidden_states[0].shape[1:])} meaning \"\n",
    "    + \"[sequence length, model dimensions]\")\n",
    "print(\n",
    "    f\"Last hidden state: has shape {list(outputs.last_hidden_state[0].shape)} meaning \"\n",
    "    + \"[sequence length, model dimensions]\")\n",
    "print(\n",
    "    f\"Pooler output: a 1D vector of {outputs.pooler_output[0].shape[0]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand inputs, layers, and outputs of a transformers model, it would be interesting to visualise where in embedding space the input vectors start and where they end up after the transformers applies the contextual and semantic mapping that it learnt during pre-training. \n",
    "\n",
    "We'll use PCA and t-SNE to reduce the dimensionality to two for plotting. t-SNE can create reductions with different non-linear relationships, so we'll make a few plots, sampling different numbers of components, as well as PCA-only plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(tsne_components: List[int]):\n",
    "\n",
    "    # Feature IDs (scalar)\n",
    "    word_ids = token_ids.input_ids[0]\n",
    "    sentence_ids = token_ids.token_type_ids[0]\n",
    "    position_ids = list(range(len(word_ids)))\n",
    "\n",
    "    # Embedding Lookup Matrices\n",
    "    word_embedding_matrix = model.embeddings.word_embeddings.weight.detach().numpy()\n",
    "    sentence_embedding_matrix = model.embeddings.token_type_embeddings.weight.detach().numpy()\n",
    "    position_embedding_matrix = model.embeddings.position_embeddings.weight.detach().numpy()\n",
    "\n",
    "    # Input Embeddings\n",
    "    word_embeddings = word_embedding_matrix[word_ids]\n",
    "    sentence_embeddings = sentence_embedding_matrix[sentence_ids]\n",
    "    position_embeddings = position_embedding_matrix[position_ids]\n",
    "    input_token_embeddings = outputs.hidden_states[0][0].detach().numpy()\n",
    "\n",
    "    # Meaningful Output Embeddings\n",
    "    output_token_embeddings = outputs.last_hidden_state[0].detach().numpy()\n",
    "\n",
    "    # Combine all lists\n",
    "    data = np.concatenate((\n",
    "        word_embeddings, sentence_embeddings, position_embeddings,\\\n",
    "        input_token_embeddings, output_token_embeddings), axis=0)\n",
    "\n",
    "    # Define the lengths of the embeddings\n",
    "    lengths = [\n",
    "        len(word_embeddings), len(sentence_embeddings), len(position_embeddings), \\\n",
    "        len(input_token_embeddings), len(output_token_embeddings)]\n",
    "\n",
    "    # Define the plot size\n",
    "    size = 10\n",
    "    alpha = 0.3\n",
    "\n",
    "    # Create subplots, one for PCA and other for each number of components in t-SNE\n",
    "    tsne_components.insert(0, None) # plus one for PCA\n",
    "    num_rows = len(tsne_components) // 2 + len(tsne_components) % 2\n",
    "    fig, axs = plt.subplots(num_rows, 2, figsize=(15, 5 * num_rows))\n",
    "\n",
    "    # Create t-SNE plots\n",
    "    for i, n_components in enumerate(tsne_components):\n",
    "\n",
    "        # Reduce dimensionality to 2D for plotting\n",
    "        if i == 0:\n",
    "            # Use PCA to reduce dimensionality to 2D\n",
    "            pca = PCA(n_components=2)\n",
    "            plot_data = pca.fit_transform(data)\n",
    "\n",
    "            title = f'PCA to 2D'\n",
    "        else:\n",
    "            # Use PCA to reduce dimensionality to size that t-SNE can handle\n",
    "            pca = PCA(n_components=n_components)\n",
    "            data_pca = pca.fit_transform(data)\n",
    "\n",
    "            # Use t-SNE to reduce dimensionality to 2D for plotting\n",
    "            tsne = TSNE(n_components=2)\n",
    "            plot_data = tsne.fit_transform(data_pca)\n",
    "\n",
    "            title = f'PCA to {n_components}, t-SNE to 2D'\n",
    "\n",
    "        # Split the transformed data back into separate arrays\n",
    "        plot_word, plot_sentence, plot_position, plot_input_token, plot_output_token \\\n",
    "            = np.split(plot_data, np.cumsum(lengths)[:-1])\n",
    "\n",
    "        # Sentence embedding is the same for all elements of the sequence\n",
    "        plot_sentence = np.mean(plot_sentence, axis=0).reshape(1, -1)\n",
    "\n",
    "        ax = axs[i//2, i%2]\n",
    "        ax.scatter(plot_word[:, 0], plot_word[:, 1], c='r', label='Word Embeddings', s=size, alpha=alpha)\n",
    "        ax.scatter(plot_sentence[:, 0], plot_sentence[:, 1], c='g', label='Sentence Embeddings', s=size*6, alpha=alpha)\n",
    "        ax.scatter(plot_position[:, 0], plot_position[:, 1], c='b', label='Position Embeddings', s=size, alpha=alpha)\n",
    "        ax.scatter(plot_input_token[:, 0], plot_input_token[:, 1], c='y', label='Input Token Embeddings', s=size, alpha=alpha)\n",
    "        ax.scatter(plot_output_token[:, 0], plot_output_token[:, 1], c='m', label='Output Token Embeddings', s=size, alpha=alpha)\n",
    "\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings([25, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the weights of lookup matrices of the three input embedding features were trained along with the transformer layers, so the positions we see were optimized along with the output embedding space.\n",
    "\n",
    "As expected, the input embedding tokens are represented in a similar area of embedding space as the three components that it's made of. Of those components, token embeddings seem to start off with the most variance in embedding space, with position embeddings in a tighter cluster. Interestingly, positional and token type embeddings do not take on their own spaces, but share the input embedding region that word embeddings use.\n",
    "\n",
    "The output embedding space however, seems to use a completely separate area of embedding space to represent the semantic and contextual information of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping Code - Imports and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 22:57:25.575231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-09 22:57:25.575408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-09 22:57:25.773750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-09 22:57:26.177844: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 22:57:28.690444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "\n",
    "from typing import Union, List, Dict, Tuple\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from io import StringIO\n",
    "import spacy\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import fasttext.util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import logging\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "import sys\n",
    "from types import ModuleType, FunctionType\n",
    "from gc import get_referents\n",
    "\n",
    "# Custom objects know their class.\n",
    "# Function objects seem to know way too much, including modules.\n",
    "# Exclude modules as well.\n",
    "BLACKLIST = type, ModuleType, FunctionType\n",
    "\n",
    "def getsize(obj):\n",
    "    \"\"\"sum size of object & members.\"\"\"\n",
    "    if isinstance(obj, BLACKLIST):\n",
    "        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))\n",
    "    seen_ids = set()\n",
    "    size = 0\n",
    "    objects = [obj]\n",
    "    while objects:\n",
    "        need_referents = []\n",
    "        for obj in objects:\n",
    "            if not isinstance(obj, BLACKLIST) and id(obj) not in seen_ids:\n",
    "                seen_ids.add(id(obj))\n",
    "                size += sys.getsizeof(obj)\n",
    "                need_referents.append(obj)\n",
    "        objects = get_referents(*need_referents)\n",
    "    return size\n",
    "\n",
    "def crash():\n",
    "    # Get the CPU usage\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    print(f'CPU usage: {cpu_usage}%')\n",
    "    # Get the memory usage\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    print(f'Memory usage: {memory_usage}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding and Preprocessing\n",
    "\n",
    "I initially thought that a complete list of Spanish verbs would be readily accessable from multiple dictionary websites or sources. After some searching, this does not seem to be the case, but that means we get to practice more NLP!\n",
    "\n",
    "We will instead create a list of Spanish verbs by parsing verbs from a large Spanish corpus. We'll choose a corpus that focuses on modern Spanish to analyse verbs currently in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets from Hugging Face\n",
    "\n",
    "As well as models, Hugging Face hosts a vast [database](https://huggingface.co/datasets) of labelled and unlabelled datasets that can be added to by anyone. The database can be filtered by task, language, or size.\n",
    "\n",
    "Each dataset is indexed by its *dataset ID*, and is further divided into *configurations*, which are usually split along some feature (e.g., English, Spanish, etc.), and further divided into splits (tran, test, etc.). Each dataset comes with a dataset card, which shows the type, distribution, and some examples of each feature in the dataset.\n",
    "\n",
    "<u>Accessing and Dataset</u>\n",
    "\n",
    "A dataset can be accessed via Hugging Face's python library called `datasets`, or their REST API called [Dataset Server](https://huggingface.co/docs/datasets-server/index).\n",
    "\n",
    "The REST API allows one to search and index the dataset as well as get summary statistics without downloading the data. \n",
    "\n",
    "<u>Streaming a Dataset</u>\n",
    "\n",
    "Large datasets that cannot fit into memory can be streamed via both methods, although the the python library iterator returns a single instance at a time, whereas the API allows a max of 100 instances per call.\n",
    "\n",
    "In my speed tests I found the API to be faster for downloading many instances. Note that you can specify sharding configurations that might allow the python library method to surpass the REST API's 100 instance cap.\n",
    "\n",
    "<u>Working with Datasets</u>\n",
    "\n",
    "Hugging Face has their own dataset object machinery, as well as simple methods to convert to [PyTorch](https://huggingface.co/docs/datasets/v2.16.1/use_with_pytorch#data-loading) or [TensorFlow](https://huggingface.co/docs/datasets/v2.16.1/use_with_tensorflow#data-loading) dataset objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Wikepedia's [Spanish corpus](https://huggingface.co/datasets/wikimedia/wikipedia/viewer/20231101.es), comprising all Spanish entries on Wikepedia's website.\n",
    "\n",
    "Lets get acquainted with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_dataset_summary(dataset: str, config: str, split: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints a summary of the dataset's metadata.\n",
    "    \"\"\"\n",
    "    # Get Hugging Face API token\n",
    "    with open('apis.json', 'r') as f:\n",
    "        API_TOKEN = json.load(f).get('hugging_face')\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/statistics\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"config\": config,\n",
    "        \"split\": split\n",
    "    }\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve dataset summary with status code '{response.status_code}': {response.text}\")\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve dataset summary with status code '501': {\"error\":\"Job manager crashed while running this job (missing heartbeats).\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'Job manager crashed while running this job (missing heartbeats).'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'wikimedia/wikipedia'\n",
    "config = '20231101.es'\n",
    "split = 'train'\n",
    "hf_dataset_summary(dataset, config, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the wikipedia dataset does not seem to [support](https://huggingface.co/docs/datasets-server/statistics#explore-statistics-over-split-data) this summary API operation. Below is an example of normal usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 8551\n",
      "Column 'idx' has attributes ['nan_count', 'nan_proportion', 'min', 'max', 'mean', 'median', 'std', 'histogram']\n",
      "Column 'label' has attributes ['nan_count', 'nan_proportion', 'no_label_count', 'no_label_proportion', 'n_unique', 'frequencies']\n",
      "Column 'sentence' has attributes ['nan_count', 'nan_proportion', 'min', 'max', 'mean', 'median', 'std', 'histogram']\n"
     ]
    }
   ],
   "source": [
    "dataset = 'glue'\n",
    "config = 'cola'\n",
    "split = 'train'\n",
    "summary = hf_dataset_summary(dataset, config, split)\n",
    "print(f\"Rows: {summary['num_examples']}\")\n",
    "for col in summary['statistics']:\n",
    "    print(f\"Column '{col['column_name']}' has attributes {list(col['column_statistics'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another endpoint we can use to get info on the dataset's data types and data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size when loaded into memory: 32Gb \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'description': '',\n",
       "  'citation': '',\n",
       "  'homepage': '',\n",
       "  'license': '',\n",
       "  'features': {'id': {'dtype': 'string', '_type': 'Value'},\n",
       "   'url': {'dtype': 'string', '_type': 'Value'},\n",
       "   'title': {'dtype': 'string', '_type': 'Value'},\n",
       "   'text': {'dtype': 'string', '_type': 'Value'}},\n",
       "  'builder_name': 'parquet',\n",
       "  'dataset_name': 'wikipedia',\n",
       "  'config_name': '20231101.es',\n",
       "  'version': {'version_str': '0.0.0', 'major': 0, 'minor': 0, 'patch': 0},\n",
       "  'splits': {'train': {'name': 'train',\n",
       "    'num_bytes': 32483180986,\n",
       "    'num_examples': 1841155,\n",
       "    'dataset_name': None}},\n",
       "  'download_size': 3493595869,\n",
       "  'dataset_size': 32483180986},\n",
       " 'partial': False}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hf_dataset_info(dataset: str, config: str, split: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints info of the dataset's metadata.\n",
    "    \"\"\"\n",
    "    # Get Hugging Face API token\n",
    "    with open('apis.json', 'r') as f:\n",
    "        API_TOKEN = json.load(f).get('hugging_face')\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/info\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"config\": config,\n",
    "        \"split\": split\n",
    "    }\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve dataset summary with status code '{response.status_code}': {response.text}\")\n",
    "    return response.json()\n",
    "\n",
    "dataset = 'wikimedia/wikipedia'\n",
    "config = '20231101.es'\n",
    "split = 'train'\n",
    "dataset_info = hf_dataset_info(dataset, config, split)\n",
    "\n",
    "print(f\"Dataset size when loaded into memory: {round(dataset_info['dataset_info']['dataset_size']/10**9)}Gb\", '')\n",
    "dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite large (32Gb), so we won't be able to store it locally or load it all into memory; any processing will have to be done iteratively.\n",
    "\n",
    "Of the four features, `text` is the only useful one for us. Lets see how large each text is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_hf_corpus(batch_size: int = 100, offset: int = 0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns an iterable that retrieves a list of sentences\n",
    "    from the wikipedia's spanish dataset on HuggingFace.\n",
    "        Once the end of the dataset is reached, it will return an empty list []\n",
    "        for every subsequent call.\n",
    "    \"\"\"\n",
    "    dataset = 'wikimedia/wikipedia'\n",
    "    config_name = '20231101.es'\n",
    "    split = 'train'\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "    batch_size = min(batch_size, 100) # Max batch_size is 100\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"dataset\": dataset,\n",
    "            \"config\": config_name,\n",
    "            \"split\": split,\n",
    "            \"offset\": offset,\n",
    "            \"length\": batch_size\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        results = [instance[\"row\"][\"text\"] for instance in data[\"rows\"]]\n",
    "        yield results\n",
    "\n",
    "        offset += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      1000.000000\n",
      "mean      17203.568000\n",
      "std       25786.696636\n",
      "min         110.000000\n",
      "25%        2376.750000\n",
      "50%        7436.500000\n",
      "75%       20818.000000\n",
      "max      226923.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_iterator = iter_hf_corpus()\n",
    "text_lengths = []\n",
    "\n",
    "for _ in range(10):\n",
    "    text_lengths.extend([len(text) for text in next(corpus_iterator)])\n",
    "text_lengths_series = pd.Series(text_lengths)\n",
    "\n",
    "print(text_lengths_series.describe())\n",
    "text_lengths_series.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text is quite long, likely a full article instead of a single sentence. Later on we'll want to pass the texts through transformer models, which have max input sequence sizes, so we'll have to divide the text into smaller pieces.\n",
    "\n",
    "As well, since we are interested in verbs we will need a method for differentiating verbs from other words. To do this, we can use a **Parts of Speech** (**POS**) tagging model, which labels each word with its POS: noun, verb, adjective, etc. POS models can be rule based or stochastic / machine learning based. A few good POS models for the Spanish language are listed in Resources. We will use stochastic-based model as they are better able to handle ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-Speech Tagging by SpaCy\n",
    "\n",
    "SpaCy is a NLP library designed for production use. It provides out-of-the-box models, as well as a library for training models and preparing them for productions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a function to load a SpaCy model and another to perform POS on a text input and format the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_pos_lem(model_type: str = \"medium\") -> spacy.language.Language:\n",
    "    \"\"\"\n",
    "    Loads a SpaCy model for Spanish that performs:\n",
    "    - Tokenization\n",
    "    - Parts of speech tagging\n",
    "    - Dependency parcing\n",
    "    - Lemmatization\n",
    "    and disables the named entity recognition (NER) part of the model.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"small\": \"es_core_news_sm\",\n",
    "        \"medium\": \"es_core_news_md\",\n",
    "        \"large\": \"es_core_news_lg\",\n",
    "        \"transformer\": \"es_dep_news_trf\"}\n",
    "\n",
    "    ## Check if model argument is valid\n",
    "    try:\n",
    "        model_name = models[model_type]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid model name. Choose from 'small', 'medium', 'large', or 'transformer'.\")\n",
    "\n",
    "    ## Get model and download model if not installed\n",
    "    while True:\n",
    "        try:\n",
    "            model = spacy.load(model_name, disable=[\"ner\"])\n",
    "            break\n",
    "        except OSError:\n",
    "            print(f\"Installing Spacy model '{model_name}'\")\n",
    "            spacy.cli.download(model_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def spacy_pos_tagger(\n",
    "    data: Union[List[str], str], model: spacy.language.Language = None, model_type: str = None\n",
    "    ) -> np.array:\n",
    "    \"\"\"\n",
    "    Tags a text with the Part of Speech (POS) and infinitive verb of each word.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array with the following columns:\n",
    "        - text: The word itself\n",
    "        - pos: The part of speech of the word. E.g. VERB, NOUN, ADJ, etc.\n",
    "        - lemma: The infinitive verb of the word. E.g., comer, nadar, etc.\n",
    "    \"\"\"\n",
    "    ## Get SpaCy model\n",
    "    if model is None:\n",
    "        model = get_spacy_pos_lem(model_type)\n",
    "\n",
    "    ## Clean input text\n",
    "    if not isinstance(data, list):\n",
    "        docs = [model(data)]\n",
    "    else:\n",
    "        docs = model.pipe(data) # uses SpaCy's internal batching\n",
    "\n",
    "    pos = [[token.text, token.pos_, token.lemma_] for doc in docs for token in doc]\n",
    "    return pd.DataFrame(pos, columns=[\"word\", \"pos\", \"lemma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy's [docs](https://spacy.io/models/es) state a POS accuracy of 0.95 to 0.99, depending on the model. Before we use the model to create a dataset of verbs, lets confirm the model's performance using a small pre-made list of verbs as labels. I found two lists from github users; we'll use Bret's list for now. \n",
    "\n",
    "Lets inspect our dataset of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs_bret() -> List:\n",
    "    # Send a GET request to fetch the XML content\n",
    "    url = \"https://raw.githubusercontent.com/bretttolbert/verbecc/main/verbecc/data/verbs-es.xml\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the XML content\n",
    "    xml_content = response.content\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    # Parse verbs from xml content\n",
    "    verb_elements = root.findall(\".//v\")\n",
    "    verbs = [v.find(\"i\").text for v in verb_elements]\n",
    "    return verbs\n",
    "\n",
    "\n",
    "def get_verbs_ghid() -> List:\n",
    "    \"\"\"\n",
    "    Retrieves a list of verbs and definitions from ghidinelli's fred-jehle-spanish-verbs project.\n",
    "\n",
    "    Returns:\n",
    "    - A list of verbs.\n",
    "    \"\"\"\n",
    "    verbs = []\n",
    "\n",
    "    # Fetch CSV contents\n",
    "    url = 'https://raw.githubusercontent.com/ghidinelli/fred-jehle-spanish-verbs/master/jehle_verb_database.csv'\n",
    "    response = requests.get(url)\n",
    "    file = StringIO(response.text)\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    # Parse verbs from csv contents\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        verb = row[0]\n",
    "        definition = row[1] # In case I want these\n",
    "        verbs.append(verb)\n",
    "\n",
    "    verbs = list(set(verbs)) # Remove duplicates\n",
    "    return verbs\n",
    "\n",
    "\n",
    "def premade_verb_list(source: str = 'bret') -> List:\n",
    "    \"\"\"\n",
    "    Retrieves a list of verbs from one of the premade sources.\n",
    "\n",
    "    Returns:\n",
    "    - A list of verbs.\n",
    "    \"\"\"\n",
    "    if source == 'bret':\n",
    "        return get_verbs_bret()\n",
    "    elif source == 'ghid':\n",
    "        return get_verbs_ghid()\n",
    "    else:\n",
    "        raise ValueError(\"source must be one of ['bret', 'ghid']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repellar',\n",
       " 'ocupar',\n",
       " 'reconquistar',\n",
       " 'guataquear',\n",
       " 'zaherir',\n",
       " 'recibir',\n",
       " 'elijar',\n",
       " 'tipear',\n",
       " 'tarjar',\n",
       " 'trasvolar']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = premade_verb_list()\n",
    "random.sample(verbs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how well SpaCy's models perform POS on the premade list. Note two weaknesses of our test dataset:\n",
    "- Only samples the infinitive conjugation tense of verbs.\n",
    "- Instances are single words instead of sentences. POS models often perform better with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>94.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>1.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word\n",
       "pos         \n",
       "VERB   94.98\n",
       "NOUN    1.98\n",
       "PROPN   1.42\n",
       "ADJ     1.35\n",
       "AUX     0.14\n",
       "ADV     0.06\n",
       "NUM     0.03\n",
       "ADP     0.01\n",
       "INTJ    0.01\n",
       "PRON    0.01\n",
       "PUNCT   0.01"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = spacy_pos_tagger(verbs, model_type='medium')\n",
    "counts = pos_df[['pos', 'word']].groupby('pos').count()\n",
    "counts = (counts * 100 /counts.sum()).round(2).sort_values('word', ascending=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0.95 accuracy is close to the stated 0.98 for the medium size model. The difference may be due to:\n",
    "- rare verbs\n",
    "- a lack of context since the verbs are stand alone words\n",
    "- possibly some errors in bret's verb list; after some manual checking I found that some of the words do not exist on SpanishDict.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOUN': ['entreoír', 'visar', 'rapiñar', 'fallir', 'enruinecer'],\n",
       " 'PROPN': ['disturbar', 'reensayar', 'resorber', 'desencinchar', 'cinchar'],\n",
       " 'ADJ': ['desceñir', 'lancinar', 'recolar', 'hipertrofiar', 'aureolar'],\n",
       " 'AUX': ['enrabar', 'mancornar', 'amorriñar', 'enalbar', 'quintaesenciar'],\n",
       " 'ADV': ['astriñir', 'costriñir', 'gandujar', 'guañir', 'lamber', 'muñir'],\n",
       " 'NUM': ['desjarretar', 'esposar', 'reembolsar'],\n",
       " 'PUNCT': ['engurruñar'],\n",
       " 'PRON': ['gañir'],\n",
       " 'INTJ': ['lamer'],\n",
       " 'ADP': ['tractorar']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mislabelled = {\n",
    "    pos: pos_df.loc[pos_df['pos'] == pos, 'lemma'].values.tolist() \\\n",
    "    for pos in pos_df['pos'].unique() if pos !='VERB'}\n",
    "sample = {\n",
    "    tag: (words if len(words) < 10 else random.sample(words, 5)) \\\n",
    "    for tag, words in mislabelled.items()}\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that AUX is also being used for some verbs, as it contains the common verbs estar, haber, saber, and ser.\n",
    "Most of the verbs, however, seem very uncommon or simply not spanish words, so we'll move ahead with this POS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Edge Cases\n",
    "\n",
    "Some Spanish words are both verbs and have other meanings, so lets test how our model performs on these worst-case situation within sentences for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_verbs_test(model):\n",
    "    \"\"\"\n",
    "    Como / comiendno        means to eat and is also a conjunction\n",
    "    Nada / nadas / nadaba   means to swim and also means \"nothing\"\n",
    "    Pasa / pasas            means to happen and also means raisins\n",
    "\n",
    "    \"\"\"\n",
    "    # We surround the verbs in context so the model can differentiate between the verb and other meaning\n",
    "    test_phrase = '\\\n",
    "        Yo como manzanas. Siempre estoy comiendo manzanas. Ella nada los sabados \\\n",
    "        y tu nadas los domingos. Antes, ella nadaba los domingos tambien. No le \\\n",
    "        pasa nada a ella cuando come pasas'\n",
    "\n",
    "    test_words = ['como', 'nada', 'nadas', 'nadaba', 'pasa', 'pasas']\n",
    "\n",
    "    array = spacy_pos_tagger(test_phrase, model)\n",
    "    df = pd.DataFrame(array, columns=['text', 'pos', 'lemma'])\n",
    "\n",
    "    df['Success'] = df['pos'] == 'VERB'\n",
    "    df = df[df['text'].isin(test_words)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, pos, lemma, Success]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trick_verbs_test(spacy.load('es_core_news_lg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were hoping to see the model correctly label each test word as a verb, but more than half the tests failed.\n",
    "\n",
    "Transformers use Multi-Head attention layers to understand sequences in the context of the rest of the sequence, so lets try SpaCy's tranformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, pos, lemma, Success]\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trick_verbs_test(spacy.load('es_dep_news_trf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer was successful on two more of the tricks, but still makes error clear to a Spanish speaker.\n",
    " \n",
    "Thankfully there are not many verbs with other meanings, so we'll stick with the medium sized model for now to save on compute cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Test \n",
    "Next we'll parse the Wikipedia Spanish corpus with our POS model to create list of modern-use Spanish verbs.\n",
    "\n",
    "It could be interesting to see how our embedding space analysis varies with more and less common verbs. For this we'll want to make a bag-of-words, which is simply a frequency count of each word in a corpus or document.\n",
    "\n",
    "The Wikipedia dataset is to large to fit in memory, so we'll stream the dataset in chunks, and apply the POS parsing and BOW counting in the same function to avoid iterating through the large dataset twice.\n",
    "\n",
    "As well, this corpus is quite large, so we'll speed test some methods for applying POS to the streaming iterable before trying to parse the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_corpus_func() -> datasets.IterableDataset:\n",
    "    \"\"\"\n",
    "    Create's an iterable for the HuggingFace large_spanish_corpus dataset.\n",
    "        Streaming=True allows us to iterate over the dataset without loading it all into memory.\n",
    "    \"\"\"\n",
    "    dataset = 'large_spanish_corpus'\n",
    "    config_name = 'EUBookShop'\n",
    "    split = 'train'\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        dataset, name=config_name, split=split,\n",
    "        streaming=True, trust_remote_code=True)\n",
    "    return iter(dataset)\n",
    "\n",
    "def hf_corpus_api(batch_size: int = 100, offset: int = 0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns an iterable that retrieves a list of sentences\n",
    "    from the HuggingFace large_spanish_corpus dataset.\n",
    "        Once the end of the dataset is reached, it will return an empty list []\n",
    "        for every subsequent call.\n",
    "    \"\"\"\n",
    "    dataset = 'large_spanish_corpus'\n",
    "    config_name = 'EUBookShop'\n",
    "    split = 'train'\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "    batch_size = min(batch_size, 100) # Max batch_size is 100\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"dataset\": dataset,\n",
    "            \"config\": config_name,\n",
    "            \"split\": split,\n",
    "            \"offset\": offset,\n",
    "            \"length\": batch_size\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        results = [instance[\"row\"][\"text\"] for instance in data[\"rows\"]]\n",
    "        yield results\n",
    "\n",
    "        offset += batch_size\n",
    "\n",
    "\n",
    "def pos_speed_test(option: int, iterator: str, batch_size: int = 100, test_instances: int = 100):\n",
    "    \"\"\"\n",
    "    The test consists of two download methods and three POS processing methods.\n",
    "\n",
    "    Download methods:\n",
    "    - HuggingFace REST api\n",
    "    - HuggingFace python library\n",
    "\n",
    "    POS processing methods:\n",
    "    - Spacy's internal batching via .pipe()\n",
    "    - Combining instances into a single string; one sentence per instance.\n",
    "    - Simply processes each instance individually\n",
    "    \"\"\"\n",
    "    pos_model = get_spacy_pos_lem(\"medium\")\n",
    "\n",
    "    if iterator not in ['api', 'func']:\n",
    "        raise ValueError('Invalid iterator argument')\n",
    "\n",
    "    batch = []\n",
    "    results = []\n",
    "\n",
    "    instances_processed = 0\n",
    "\n",
    "    # Test process: using spacy's internal batching via .pipe()\n",
    "    if option == 1:\n",
    "        while True:\n",
    "\n",
    "            # Test download: HuggingFace REST api\n",
    "            if iterator == 'api':\n",
    "                corpus_iterator = hf_corpus_api(batch_size)\n",
    "                batch = next(corpus_iterator)\n",
    "\n",
    "            # Test download: HuggingFace python library\n",
    "            elif iterator == 'func':\n",
    "                corpus_iterator = hf_corpus_func()\n",
    "                for _ in range(batch_size):\n",
    "                    batch.append(next(corpus_iterator)['text'])\n",
    "\n",
    "            docs = pos_model.pipe(batch)\n",
    "            results.extend([token for doc in docs for token in doc])\n",
    "\n",
    "            # Stop when we've processed the number of test instances\n",
    "            instances_processed += batch_size\n",
    "            if instances_processed >= test_instances:\n",
    "                break\n",
    "\n",
    "    # Test process: combining instances into a single string; one sentence per instance.\n",
    "    elif option == 2:\n",
    "        while True:\n",
    "\n",
    "            # Test download: HuggingFace REST api\n",
    "            if iterator == 'api':\n",
    "                corpus_iterator = hf_corpus_api(batch_size)\n",
    "                batch = next(corpus_iterator)\n",
    "\n",
    "            # Test download: HuggingFace python library\n",
    "            elif iterator == 'func':\n",
    "                corpus_iterator = hf_corpus_func()\n",
    "                for _ in range(batch_size):\n",
    "                    batch.append(next(corpus_iterator)['text'])\n",
    "\n",
    "            batch_text = ' .'.join(batch)\n",
    "            doc = pos_model(batch_text)\n",
    "            results.extend([token for token in doc])\n",
    "\n",
    "            # Stop when we've processed the number of test instances\n",
    "            instances_processed += batch_size\n",
    "            if instances_processed >= test_instances:\n",
    "                break\n",
    "\n",
    "    # Test process: simply processes each instance individually\n",
    "    elif option == 3:\n",
    "        while True:\n",
    "\n",
    "            # Test download: HuggingFace REST api\n",
    "            if iterator == 'api':\n",
    "                corpus_iterator = hf_corpus_api(batch_size=1)\n",
    "                text = next(corpus_iterator)[0]\n",
    "                doc = pos_model(text)\n",
    "                results.extend([token for token in doc])\n",
    "\n",
    "                # Stop when we've processed the number of test instances\n",
    "                instances_processed += 1\n",
    "                if instances_processed >= test_instances:\n",
    "                    break\n",
    "\n",
    "            # Test download: HuggingFace python library\n",
    "            elif iterator == 'func':\n",
    "                corpus_iterator = hf_corpus_func()\n",
    "                text = next(corpus_iterator)['text']\n",
    "                doc = pos_model(text)\n",
    "                results.extend([token for token in doc])\n",
    "\n",
    "                # Stop when we've processed the number of test instances\n",
    "                instances_processed += 1\n",
    "                if instances_processed >= test_instances:\n",
    "                    break\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.94 s ± 138 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "2.17 s ± 160 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "27.4 s ± 5.33 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "\n",
      "3.33 s ± 39.9 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "3.32 s ± 58.8 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r 3 -n 1 pos_speed_test(1, 'api', 100)\n",
    "%timeit -r 3 -n 1 pos_speed_test(2, 'api', 100)\n",
    "%timeit -r 3 -n 1 pos_speed_test(3, 'api', 100)\n",
    "print()\n",
    "%timeit -r 3 -n 1 pos_speed_test(1, 'func', 100)\n",
    "%timeit -r 3 -n 1 pos_speed_test(2, 'func', 100)\n",
    "%timeit -r 3 -n 1 pos_speed_test(3, 'func', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading in batches is much faster than requesting instances one at a time via the python function.\n",
    "\n",
    "As well, streaming the instances to SpaCy's model via [language.pipe](https://spacy.io/api/language#pipe) is comparable to processing them as a single text, and comes with the benefit of keeping instances separate (which is mandatory in most NLP use cases), so we'll go with that method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Corpus and Derive Embeddings\n",
    "\n",
    "**Describe the thought process for building the download and derive embeddings script.**\n",
    "\n",
    "Ideas:\n",
    "- Now that we have a solid list of common day Spanish verbs, we can explore their representation in embedding space of some pre-trained NLP models.\n",
    "- One of the word embedding models we will use are transformer models, which are quite complex instruments, so I've written a section clarifying how data flows through a transformer to go from text to semantic and contextual embeddings, which you can find in the [Terminology and Background Info](##Terminology-and-Background-Info) section. \n",
    "- We start with a list of sentences.\n",
    "- We want a few things:\n",
    "    - List of verbs; need POS tagger\n",
    "    - Ability to identify conjugated verbs with their infinitive; need lemmatizer\n",
    "    - Every sentence that contains the verb or one of its conjugations\n",
    "    - The start and end character position of the verb or its conjugation in the sentence.\n",
    "    - The ability to note the position of a verb or its conjugation multiple times in the same sentence.\n",
    "- The POS tagger we will use is SpaCy\n",
    "- The lemmatizer we will use is SpaCY\n",
    "- The corpus is large, so we will want to iterate through it only once. What this looks like with our function processes is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings_from_corpus import EmbeddingsFromCorpus\n",
    "embedding_generator = EmbeddingsFromCorpus(data_dir='data_temp_larger')\n",
    "# embedding_generator.derive_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_context = embedding_generator.load_embeddings_with_context()\n",
    "no_context = embedding_generator.load_embeddings_no_context()\n",
    "word_counts = embedding_generator.load_word_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we'll want to do is check the embeddings we derives to make sure the process went as expected, and to see the distribution of data that we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "One or more models missing verbs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(with_context\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msorted\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_cnn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_trf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m]), \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModels missing from embeddings with context.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28msorted\u001b[39m(model_data\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msorted\u001b[39m(word_counts\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;28;01mfor\u001b[39;00m model_data \u001b[38;5;129;01min\u001b[39;00m with_context\u001b[38;5;241m.\u001b[39mvalues()), \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOne or more models missing verbs.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: One or more models missing verbs."
     ]
    }
   ],
   "source": [
    "assert sorted(with_context.keys()) == sorted(['spacy_cnn', 'spacy_trf', 'bert', 'gpt2']), \\\n",
    "    'Models missing from embeddings with context.'\n",
    "assert all(sorted(model_data.keys()) == sorted(word_counts.keys()) for model_data in with_context.values()), \\\n",
    "    'One or more models missing verbs.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print the discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name      | Missing Verbs  \n",
      "spacy_cnn       | []\n",
      "spacy_trf       | []\n",
      "bert            | ['coucher ']\n",
      "gpt2            | ['temer', 'coucher ']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model Name':<15} | {'Missing Verbs':<15}\")\n",
    "for model_name, model_data in with_context.items():\n",
    "    missing_verbs = [key for key in word_counts.keys() if key not in model_data.keys()]\n",
    "    print(f\"{model_name:<15} | {missing_verbs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well, lets make sure we only got verbs, and verbs without punctuation or white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name      | Verbs with Punctuation or Whitespaces\n",
      "spacy_cnn       | ['www.elcodigoascii.com.ar', 'al-manazir', 're-clasificar', 're)distribuir', 'agrícola-cultural-comercial-militar', 'bi-locular', 'j.v.suringar', 'sizeof(char', '/*notar', 'corroer ', 'sistemas[especificar', 'catalán-valenciano-balear', 'pre-calcular', 'bar-bar', 'coucher ']\n",
      "spacy_trf       | ['www.elcodigoascii.com.ar', 'al-manazir', 're-clasificar', 're)distribuir', 'agrícola-cultural-comercial-militar', 'bi-locular', 'j.v.suringar', 'sizeof(char', '/*notar', 'corroer ', 'sistemas[especificar', 'catalán-valenciano-balear', 'pre-calcular', 'bar-bar', 'coucher ']\n",
      "bert            | ['www.elcodigoascii.com.ar', 'al-manazir', 're-clasificar', 're)distribuir', 'agrícola-cultural-comercial-militar', 'bi-locular', 'j.v.suringar', 'sizeof(char', '/*notar', 'corroer ', 'sistemas[especificar', 'catalán-valenciano-balear', 'pre-calcular', 'bar-bar']\n",
      "gpt2            | ['www.elcodigoascii.com.ar', 'al-manazir', 're-clasificar', 're)distribuir', 'agrícola-cultural-comercial-militar', 'bi-locular', 'j.v.suringar', 'sizeof(char', '/*notar', 'corroer ', 'sistemas[especificar', 'catalán-valenciano-balear', 'pre-calcular', 'bar-bar']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def check_for_punctuation_whitespace(lst):\n",
    "    dirty_verbs = []\n",
    "    for verb in lst:\n",
    "        for char in verb:\n",
    "            if char in string.punctuation or char in string.whitespace:\n",
    "                dirty_verbs.append(verb)\n",
    "                break\n",
    "    return dirty_verbs\n",
    "\n",
    "print(f\"{'Model Name':<15} | {'Verbs with Punctuation or Whitespaces':<15}\")\n",
    "for model_name, model_data in with_context.items():\n",
    "    dirty_verbs = check_for_punctuation_whitespace(model_data.keys())\n",
    "    print(f\"{model_name:<15} | {dirty_verbs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was surprised to see SpaCy's tokenizer return some of these strings, and as well to classify them as Verb or Auxillary word with its POS tagger, but that's why we check! We'll remove the wrongly labelled strings, and the strings that all models were not able to get embeddings for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs in Model |Embeddings with Context  |Embeddings without Context\n",
      "spacy_cnn      |1921                     |1921\n",
      "spacy_trf      |1921                     |1921\n",
      "bert           |1921                     |1921\n",
      "gpt2           |1921                     |1921\n"
     ]
    }
   ],
   "source": [
    "# Verbs missing in any model\n",
    "common_verbs = set(word_counts.keys())\n",
    "all_verbs = list(word_counts.keys())\n",
    "for key in with_context.keys():\n",
    "    common_verbs &= set(with_context[key].keys())\n",
    "    common_verbs &= set(no_context[key].keys())\n",
    "    all_verbs.extend(with_context[key].keys())\n",
    "    all_verbs.extend(no_context[key].keys())\n",
    "common_verbs = list(common_verbs)\n",
    "all_verbs = list(set(all_verbs))\n",
    "missing_verbs = list(set(all_verbs) - set(common_verbs))\n",
    "\n",
    "# Remove dirty verbs and missing verbs from word counts and embeddings\n",
    "verbs_to_remove = dirty_verbs + missing_verbs\n",
    "def remove_verbs(verbs_to_remove, embeddings):\n",
    "    for model_data in embeddings.values():\n",
    "        for verb in verbs_to_remove:\n",
    "            model_data.pop(verb, None)\n",
    "    return embeddings\n",
    "with_context = remove_verbs(verbs_to_remove, with_context)\n",
    "no_context = remove_verbs(verbs_to_remove, no_context)\n",
    "\n",
    "print(\n",
    "    f\"{'Verbs in Model':<15}|\"\n",
    "    f\"{'Embeddings with Context':<25}|\"\n",
    "    f\"{'Embeddings without Context'}\")\n",
    "for model_name in with_context.keys():\n",
    "    print(\n",
    "        f\"{model_name:<15}|\"\n",
    "        f\"{len(with_context[model_name]):<25}|\"\n",
    "        f\"{len(no_context[model_name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "So far, we've downloaded a corpus of Spanish texts, used Parts-of-Speech tagging to and some simple regex to find verbs, used a lemmatizer to match all conjugations of a verb together and label them under the infinitive version, and derive the embeddings of each verb and its conjugations, every time it was seen in the corpus, and for the infinitive verb itself, for a few different word embedding models.\n",
    "\n",
    "Now its time to explore the embedding space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to answer:\n",
    "- Do verbs in different contexts take up a large embedding space or are they tightly clustered\n",
    "    - For each model\n",
    "- Do ar, er, ir verbs cluster?\n",
    "- How far are the infinitive forms from their with context clusters\n",
    "- Are similar meaning verbs similar represented in similar space?\n",
    "- Are re verbs close to their non-re counterparts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from embeddings_from_corpus import EmbeddingsFromCorpus\n",
    "\n",
    "corpus = EmbeddingsFromCorpus(data_dir='data')\n",
    "\n",
    "try:\n",
    "    corpus.derive_data(sample_size=500000)\n",
    "except Exception:\n",
    "    corpus.logger.error(\"Exception occurred\", exc_info=True)\n",
    "    print(\"Failed to derive data from corpus: all samples.\")\n",
    "    try:\n",
    "        corpus.derive_data(sample_size=500000)\n",
    "    except Exception:\n",
    "        corpus.logger.error(\"Exception occurred\", exc_info=True)\n",
    "        print(\"Failed to derive data from corpus: 500k samples.\")\n",
    "        corpus.derive_data(sample_size=100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
