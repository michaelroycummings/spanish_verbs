{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish Verbs in Embedding Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This section serves as the manager of the project, and will cover the following topics:\n",
    "- **Aim and Goals**: clarify the focus of the project.\n",
    "- **Plan**: clearly state each step of the project.\n",
    "    - Although this seems a little self explanatory, exploratory data analysis (EDA) heavy projects like this one can easily rabbit-hole. The plan will be iterated on continuously and serve to determine whether a question that arises from EDA helps the final goal, before that question is explored and possibly branched further into new questions. \n",
    "- **Resources**: serves as a link lookup for each data needed, and allows us to verify whether we can access all the data necessary for our current plan before getting halfway and realizing data constraints mean we need to change the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim and Goals\n",
    "This project has a very simple idea: in English, verbs that start with \"re\", that also have a non-re counterpart (e.g., redo and do) mean \"to do the verb again\".\n",
    "As I've been learning Spanish over the past few months, I've noticed verbs that do and do not follow this rule, so the project aims to use the embedding space of pre-trained language models to explore the adherence of Spanish verbs to this rule.\n",
    "\n",
    "Although the aim is simple, this is my first project after a year of self studying my Master's in Data Science, so this project will be overly verbose, branching into side ideas to practice and iron out the application kinks in as many topics as I can.\n",
    "\n",
    "An adaptive list:\n",
    "- practice matrix math with embedding space\n",
    "- practice creating language model\n",
    "- practice using pretrained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "1. We need a Parts-of-Speech model to label a Spanish corpus as verbs or not.\n",
    "    1. Will need to test POS model's ability before using it to get a large list of verbs.\n",
    "        1. Find premade list of verbs.\n",
    "        2. Test POS performance on premade verb list.\n",
    "2. Get Spanish Verbs\n",
    "    1. Find Spanish Corpus\n",
    "    2. Use POS to filter for Verbs.\n",
    "    3. Create Bag-of-Words\n",
    "3. Get embedding models.\n",
    "    - Explain how transformers create embeddings from text using Hugging Face and BERT\n",
    "    - Hugging Face Transformer\n",
    "    - SpaCy tok2vec component\n",
    "    - Fasttext: will be baseline\n",
    "    - Maybe use glove or word2vec as other baselines\n",
    "4. Create baseline distance:\n",
    "    - Min and max distance between verbs\n",
    "    - Standard deviation of distance between verbs\n",
    "    - Average distance of kth closest verb for first 30 k's\n",
    "        - For every verb, calculate distance of 30 closest verbs, then average over all verbs\n",
    "    - Sanity check: Average distance between a few verbs and their similarity verb clusters\n",
    "5. Create tuples of re verbs and their non re counterparts\n",
    "6. Tests on RE verbs:\n",
    "    - 2D feature reduction:\n",
    "        - plot tuples in 2D space\n",
    "    - Embedding distance between tuples\n",
    "        - Min, max, mean, std dev\n",
    "        - plot distribution\n",
    "        - plot distance vs length of verb\n",
    "        - plot distance vs use of verbs (which metric to combine uses? average or max or both?)\n",
    "    - Kth closest verb for each item of each tuple\n",
    "    \n",
    "\n",
    "Other: should I add sense2vec or other sense vector models and compare the words, somehow, in each sense? At least mention sense vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "<u>Spanish Corpora and Word Lists</u>\n",
    "\n",
    "| Type       | Source        | Link |\n",
    "|------------|---------------|------|\n",
    "| Corpus     | Kaggle        | [120M Words](https://www.kaggle.com/datasets/rtatman/120-million-word-spanish-corpus) |\n",
    "| Corpus     | HuggingFace   | [LargeSpanishCorpus](https://huggingface.co/datasets/large_spanish_corpus) |\n",
    "| Corpus     | HuggingFace   | [SpanishBillionWords](https://huggingface.co/datasets/spanish_billion_words) |\n",
    "| Words      | Github        | [lorenbrichter](https://raw.githubusercontent.com/lorenbrichter/Words/master/Words/es.txt) |\n",
    "| Verbs      | Github        | [bretttolbert](https://github.com/bretttolbert/verbecc/blob/main/verbecc/data/verbs-es.xml) |\n",
    "| Verbs      | Github        | [ghidinelli](https://github.com/ghidinelli/fred-jehle-spanish-verbs/blob/master/jehle_verb_database.csv) |\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Parts of Speech Checkers</u>\n",
    "- Hard coded: ar, er, ir\n",
    "- HuggingFace: [Spanish RoBERTa](https://huggingface.co/PlanTL-GOB-ES/roberta-large-bne-capitel-pos)\n",
    "- Spacy: [Spanish pipelines](https://spacy.io/models/es#es_core_news_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things Learnt\n",
    "1. Transformer's have contextual embeddings (think multi-head attention), so they cannot provide an embedding for stand-alone words.\n",
    "2. some verbs have .pos_ 'AUX' not 'VERB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things I Would Do With More Time\n",
    "1. Try/except + logging clauses for code that downloads the corpus data and derives the word embeddings.\n",
    "    - On each instance in case some instances are not cleaned or have quirks\n",
    "    - On each REST request, as these do not always execute perfectly (e.g., signal drops, rate limit hit, etc.)\n",
    "    * This would add a step during analysis to ensure models have similar sample sizes of embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology and Background Info\n",
    "As many topics in natural language processing (NLP) are new and evolving, terminology is sometimes used to mean multiple things, which is detrimental to non-experts (like me) who do not have the context to assume the true meaning in each case. We will set clear definitions for these terms to avoid confusion throughout the project.\n",
    "\n",
    "As well, we will briefly explain model concepts so that decisions and comparisons made throughout the project can be appreciated by non-experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarifying Common Terms\n",
    "Below is a clarification of some terminology that I've often seen used a little less specifically than is useful for people entering the space.\n",
    "\n",
    "Processes:\n",
    "- **Encoding:**\n",
    "  - **Meaning:** Tokenization and then numericalization. Takes text data, tokenizes it into string or byte tokens, and outputs a numerical scalar, called a token ID, for each token in the input sequence.\n",
    "  - **Conflated with:** Sometimes used to mean just tokenization.\n",
    "- **Tokenization:**\n",
    "  - **Meaning:** Splitting text data into smaller text data or into byte-data based on some rules.\n",
    "  - **Conflated with:** Used with about a 50 / 50 to mean solely tokenization, or tokenization and numericalization.\n",
    "- **Numericalization:**\n",
    "  - **Meaning:** Mapping a token to a numerical scalar called a token ID.\n",
    "  - **Conflated with:** This is almost never mentioned, but its crucial to understanding how the feature space of ML embedding models begin.\n",
    "- **Embedding Lookup:**\n",
    "  - **Meaning:** Mapping a token ID to position in multidimensional feature space, which we will call **input token embeddings**. These non-learnable embeddings are the tokens of the tokenizer, in the language of the embedding model. They are non-learnable, stored in an embedding matrix, and made based on a single vocabulary, to be sufficiently separated so that the embedding model can learn how to associate the tokens without a preconceived bias in the form of some tokens being closer together than others.\n",
    "  - **Conflated with:** This is also almost never mentioned, but it is so crucial! To understanding how the feature space of ML embedding models begin.\n",
    "    - **Note:** The term token embedding or word embedding is conflated constantly: when talking about the parts of a model, input word embeddings are simply called word embeddings. But when talking about uses of embedding models, word embeddings are the output of the embedding model for each token. So we will continue using input and output to distinguish the two. \n",
    "- **Embedding:**\n",
    "  - **Meaning:** Mapping a numerical vector that lacks semantic or contextual meaning (an input token embedding) to a numerical vector with semantic or contextual meaning (an output token embedding).\n",
    "  - **Conflated with:** Sometimes used to mean tokenization.\n",
    "\n",
    "Algorithms:\n",
    "- **Tokenizer:**\n",
    "  - **Meaning:** When the tokenizer is stand-alone and is not used for a specific model, it means an algorithm that solely performs tokenization. When the tokenizer is meant to be used for a specific, pre-trained model, it usually means an algorithm that includes the tokenization and numericalization steps, as the word embedding model almost never takes tokens in the form of strings or bytes.\n",
    "  - **Conflated with:** Sometimes used to mean an algorithm that performs the whole pipeline of tokenization, numericalization, embedding lookup, and embedding.\n",
    "- **Embedding model**:\n",
    "- **Language model**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder vs Decoder\n",
    "\n",
    "The distinction between encoder and decoder has different meanings even inside ML. In terms of **autoencoders**, an encoder learns a mapping from input feature space to a lower dimensional feature space, and a decoder learns to reconstruct the original feature space from the lower dimensional space.\n",
    "\n",
    "This is the understanding of encoders and decoders that I see most often, and its proliferation is shown in AI chatbots. When I asked Bing copilot to explain the difference between encoder and decoders, and have it align with GPT being decoder only and BERT being encoder only, it was unable to deviate from this common explanation.\n",
    "\n",
    "In the context of sequential data however:\n",
    "- **Encoders** are bidirectional and non auto-regressive. I.e., models that have access to the entire sequence before yielding an output that will be used.\n",
    "- **Decoders** are unidirectional and auto-regressive. I.e., models that have access to only previous elements of a sequence before yielding an output that will be used.\n",
    "    - During training, auto-regressive models use **teacher forcing**, which means using the next element of the ground-truth labels as input instead of the model's last output, to avoid compounding errors. Giving future elements to the auto-regressive model would \"cheat\", i.e., would train the model in an environment that will not exist during application.\n",
    "    - During prediction, future elements of the sequence do not exist, so a decoder cannot use the whole sequence.\n",
    "\n",
    "Here are some examples to solidify the definitions:\n",
    "- **Encoder-decoder RNN model**: this model type is often used for neural machine translation (NMT), i.e., using neural networks to translate between languages. The encoder RNN takes one element of the sequence per timestep, but its output (specifically, the hidden state of its highest layer) is not used until the encoder RNN sees the entire input sequence. The decoder RNN also takes one element of the sequence per time step, and its output is used as every timestep, while taking the output of the encoder RNN, that has used the entire encoder input sequence, at every timestep.\n",
    "- **A Seq2Seq RNN decoder**: to derive a single value from a sequence, you may use an Seq2Seq RNN instead of a Seq2Vec to give more comparisons to the loss function. In this case, the model is producing an output for every (or at least some) of the elements of the input sequence, but only the final output value will be used, and the model will have seen the entire input sequence before yielding this value, so this is the design of an encoder.\n",
    "- **GPT**: a unidirectional, auto-regressive transformer model that uses masked self-attention. In the [original paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) it used next word prediction for pre-training.\n",
    "- **BERT**: BERT and [its variants](https://www.tensorflow.org/text/tutorials/classify_text_with_bert#loading_models_from_tensorflow_hub) are bidirectional, non auto-regressive transformer models that create an embedding based on the entire input sequence before an output layer or component uses that embedding for some task.\n",
    "\n",
    "Note that encoder-only models like BERT, and decoder-only models like GPT, can both perform classification and natural language generation tasks:\n",
    "- GPT's auto-regressive architecture naturally lends itself to natural language prediction, but its final output vector, the *extract* token in the [original paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) can be used as an embedding of the entire input sequence for classification tasks.\n",
    "- BERT bidirectional architecture naturally lends itself to classification tasks, but the first element of its output vector, the *CLS* token in the [original paper](https://arxiv.org/pdf/1810.04805.pdf), can be fed to a decoder model for text generation tasks like text summation.\n",
    "\n",
    "Maybe links to add:\n",
    "- lack of congruity on the topic [here](https://datascience.stackexchange.com/questions/118260/chatgpts-architecture-decoder-only-or-encoder-decoder)\n",
    "- T5 paper, section 3.2.1, discussing model structures [here](https://arxiv.org/pdf/1910.10683.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of an ML NLP Model\n",
    "\n",
    "\n",
    "Trainable NLP models can be thought of as three distinct parts:\n",
    "- **Tokenizer model**: a set of pre-defined rules that performs normalization (cleaning raw text), tokenization (dividing cleaned text data it into string or byte tokens) and numericalization (maps each token to numerical scalar called a **token ID**). \n",
    "- **Embedding model**: a embedding matrix that performs embedding lookup (each token ID is mapped to an input embedding token in embedding space) followed by a set of a set of neural layers that performs embedding (maps input token embeddings to output token embeddings in a learnable embedding space that represents features of speech, with respect to that embedding lookup mapping).\n",
    "- **Task head**: one or more neural layers that map the output token embeddings to the output(s) needed for a specific task.\n",
    "\n",
    "Each pre-trained model has a specific tokenizer that preprocessed the data before training. An embedding model can only be used with its own tokenizer because the embedding model understands language in the context of the input token embeddings, and the map from a language to those embeddings are stored in the tokenizer.\n",
    "\n",
    "A single training instance is usually one or more sentences. Each token in that instance is an element of the sequence for that instance.\n",
    "\n",
    "As well as token IDs, some tokenizers will pass position IDs and mask IDs to the embedding model. The embedding model will have a separate embedding lookup matrix for each type of ID. Each matrix will output the same number of dimensions, and the input token embeddings will be a sum of the embedding for each ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Models\n",
    "\n",
    "The title to this section is vague as many models fall into multiple categories: tokenizers, embedding models,  language models.\n",
    "\n",
    "<u>Ignores Semantics and Context</u>\n",
    "- Models that cannot capture semantics nor context are used solely for tokenization.\n",
    "- Examples: bag of words (BOW), term frequency inverse document frequency (TF-IDF).\n",
    "\n",
    "<u>Captures Semantics, Ignores Context</u>\n",
    "- Current models that capture semantics are neural networks comprised solely of simple feed forward layers. They are mainly used to tokenize and embed text before giving those embeddings to a more complex model for further embedding and task output, but can also be used themselves for non natural language generation tasks.\n",
    "- Examples: Word2Vec, GloVe, FastText\n",
    "\n",
    "<u>Captures Semantics and Context</u>\n",
    "- Current models that capture both semantics and context are neural networks comprised of layers that can combine multiple elements of a multidimensional sequence, i.e., recurrent, convolutional, and attention layers (the distinction here being that feed forward layers handle elements of a sequence separately).\n",
    "- They are mainly used to tokenize, embed, and then perform NLP tasks that can involve natural language generation tasks, but can also be used solely for their embeddings.\n",
    "- Examples:\n",
    "    - Encoders like BERT and its variants will usually use a task head of simple feed forward layers for classification tasks, and a complex decoder for natural language generation tasks, giving both task heads as input, the final embedding of a single token, usually a special token that does not correspond to any input text data  (e.g., CLS for BERT).\n",
    "    - Decoders like GPT will usually also use a task head of simple feed forward layers for classification tasks, and its sequence of outputs for natural language generation tasks. For the classification tasks, a task head would also receive the final embedding of a single, special token (e.g., the extract token for GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "Now that we have a good understanding of the dataflow through a transformer model, lets discuss word embeddings.\n",
    "\n",
    "**Word embeddings** are multidimensional vectors that should carry semantic structures of language, as well as contextual structures if the model is contextual. The reasoning is that, if a neural network is trained on one or more tasks that involve understanding speech, the hidden layers are likely capturing structures present in that language.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Extracting Word Embeddings</u>\n",
    "\n",
    "Since different architectures have different methods for mapping hidden layer outputs to the output layer and thus to the loss function, how we derive word embeddings is architecture dependent:\n",
    "- **Word2Vec**: both model types - CBOW and skip-gram - use a single feed forward hidden layer that is unique in the sense that it is connected to every output of every sequence element of the input layer, instead of the canonical dense layer practice of handling sequence elements independently. The output layer is connected to all neurons of the hidden layer, so the hidden layer represents semantic structure of the entire input text. Thus, we will pass single words to this model and use its entire hidden layer as the word embedding for the input word.\n",
    "- **FastText**: their models upgrade the tokenizer to a subword approach but use the same architecture as Word2Vec models, so we will derive their word embeddings in the same way.\n",
    "- **SpaCy models**: connects a word embedding component called `tok2vec` to multiple task-specific components that each use the `tok2vec`'s output as well as the output of necessary task-specific components. Thus, the `tok2vec` component is trained on multiple tasks. SpaCy employs a CNN and transformer `tok2vec` implementation, both of which have all of their token elements used in the task specific layers. This is in comparison to BERT and GPT-like architectures that use a single token for supervised learning tasks. Thus, we will take only the final layer embeddings of these models as the token vectors.\n",
    "- **Transformers**: these models often use the embeddings of a single, special token (e.g. CLS for BERT or \"predict\" for GPT) that does not represent any input data. This token has the possibility of being influenced by any token embedding at any transformer layer, with higher layers having a more likely influence. Thus, common practice for deriving word embeddings from transformers is to sum the embeddings of the last few layers for each token that makes up the target word in the input.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Contextually Dependent or Independent Word Embeddings</u>\n",
    "\n",
    "It could be interesting to compare the embedding space of context-independent and dependent word embedding models, so we will use both types in our analysis.\n",
    "\n",
    "Inputs:\n",
    "- To the context-independent models, we will input sole verbs and document the embedding space for each verb.\n",
    "- To the context-independent models, we will input each sample instance found in the corpus, and a verb embedding will be the average of that word embedding in each sentence containing the word. As well, we will note the word counts incase very infrequent verbs change the results of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText\n",
    "\n",
    "The **FastText model** is the skip-gram model using a [subword vocabulary]((https://arxiv.org/pdf/1607.04606.pdf)) made of all n-grams found in the training corpus, where n is a single choice between 3-6. Each word is represented as the sum of it's n-gram's token embeddings.\n",
    "\n",
    "The [original](https://arxiv.org/pdf/1310.4546.pdf) **skip-gram model** uses a single feed forward hidden layer that is fully connected to each feature of each element in the input sequence. Note this difference from how PyTorch and Tensorflow implement feed forward layers, which is to have it fully connected solely to the last dimension of the input shape, which results in applying the layer independently to each element of a sequence passed to the layer.\n",
    "\n",
    "As an example of the subword tokenizer, the word `happy` for a FastText model of n=4 is represented as the sum of the token embeddings for the tokens `<hap`, `happ`, `appy`, `ppy>`.\n",
    "\n",
    "FastText hosts models [pre-trained](https://fasttext.cc/docs/en/crawl-vectors.html) with the following specs:\n",
    "- Used the Common Crawl and Wikipedia text databases.\n",
    "- Used the CBOW pre-training task with a window of 5 context words and 10 negative samples.\n",
    "- Hidden layer has 300 neurons so each word embedding is 300D.\n",
    "- Tokenizer used a 5-gram subword vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy\n",
    "\n",
    "SpaCy offers two types of ready-made models:\n",
    "- [Pipeline models](https://spacy.io/models#design-cnn): models labelled `sm`/`md`/`lg` are a pipeline of [single-task](https://spacy.io/usage/processing-pipelines) CNN components that inherit from the (1) the tokenizer and (2) the last component output ([visual](https://spacy.io/models#design-cnn)). The exception being the Named Entity Recognition component which has its own trainable tokenizer. Thus, we will use the tokenizer trained on multiple tasks.\n",
    "- [Transformer models](https://spacy.io/models#design-trf): models labelled `trf` are pipeline models that have only two differences from their non-transformer counterparts: (1) the tokenizer is transformer, and (2) the NER component does not have its own tokenizer. Thus, the only choice for non-task specific embedding layers is the transformer tokenizer component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Models from HuggingFace \n",
    "\n",
    "Hugging Face is a machine learning platform that hosts an open-source database of pre-trained transformer models. Each model on the hub has a model card, that explain can explain the model's architecture, pre-training tasks, and usage via Hugging Face's `transformers` python library.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Loading and Usage</u>\n",
    "\n",
    "Many users create model cards that point to the pre-trained model file but do not create functionality for the transformers library. If the model is configured with the library, it can be used out-of-the-box via the [pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) functionality, or handled with more flexibility via an `AutoModel` [class](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel).\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Fine-Tuning</u>\n",
    "\n",
    "Models configured with `transformers` can be fine-tuned using the in-house `Trainer` [class](https://huggingface.co/docs/transformers/v4.37.1/en/main_classes/trainer#transformers.Trainer), or easily converted to work within the [PyTorch](https://pytorch.org/hub/huggingface_pytorch-transformers/) and [TensorFlow](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) python library ecosystems.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Preprocessing</u>\n",
    "\n",
    "Each model is associated with the specific tokenizer that was used for pre-training. \n",
    "\n",
    "The tokenizer outputs scalar IDs, and the transformer model's first layer is an embedding layer that performs embedding lookup to map each ID to an embedding vector. Hugging Face tokenizers output token IDs and attention IDs, as well as token type IDs for some models. The subsequent transformer model will also require position IDs, which it assigns itself, and possible language IDs, which users currently have to prepare manually.\n",
    "\n",
    "Each ID type is described below.\n",
    "\n",
    "| ID Type       | Description |\n",
    "| ------------- | ----------- |\n",
    "| Token IDs     | These map each token string or byte to a scalar integer. |\n",
    "| Token Type IDs| Used in models like BERT to differentiate between sentences in a pair. |\n",
    "| Attention IDs | Used to mask out padding tokens, to create attention windows, or for masked attention in auto-regressive models.|\n",
    "| Position IDs  | Only used for bidirectional / non auto-regressive models.|\n",
    "| Language IDs  | Only used for multilingual or cross-lingual models (e.g., [XLM](https://arxiv.org/pdf/1901.07291.pdf)) and currently, must be manually created.|\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Layers and Outputs - A Deep-ish Dive</u>\n",
    "\n",
    "**BERT** or \"Bidirectional Encoder Representations from Transformers\" is a popular bidirectional / non-masked transformer model, whose [architecture](https://arxiv.org/pdf/1810.04805.pdf) has remained the same in many newer models that improve the original training methods and training tasks used ([RoBERTa](https://arxiv.org/pdf/1907.11692.pdf), [XLM]()) or whose parameter sharing allows for a smaller model ([ALBERT](https://arxiv.org/pdf/1909.11942.pdf)).\n",
    "\n",
    "We will use BERT to understand how to access components, layers, attention matrices, and outputs of a model in the Hugging Face `transformer` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(\n",
    "    model_name, output_hidden_states = True,\n",
    "    output_attentions = True, return_dict = True)\n",
    "\n",
    "example_text = \"El gato está en la caja. El perro lo busca a el.\" # The cat is in the box. The dog looks for him.\n",
    "example_text = \"El gato está en la caja. El perro lo busca a el. Hace dos dias habia mucha gente que bailan por toda la noche y al final, todo se fue feliz. Tambien, el dijo a todo que tuvo un gran tiempo.\" # The cat is in the box. The dog looks for him.\n",
    "token_ids = tokenizer(example_text, return_tensors=\"pt\") # returns PyTorch tensors\n",
    "outputs = model(**token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer used for BERT implements **WordPiece** tokenization, a sub-word tokenization method that starts with a vocabulary of all characters in the training data, and for k steps, merges two tokens in the vocabulary, including merged tokens, to create the next most seen sub-word. BERT's tokenizer then adds a CLS token to the start of each sequence, and a SEP token between each sentence in a single sample instance (relevant for tasks like next sentence prediction and sentence comparison). Then the tokenizer performs padding and truncation if necessary, and then maps each str token to a integer scalar, called a token ID. Finally, it outputs three features for each token: token IDs, position IDs, and attention IDs.\n",
    "\n",
    "Even though we have two sentences, our model does not have a task head that can handle next sentence prediction, so the corresponding tokenizer does not differentiate between the sentences, and gives tokens in both sentences the same attention ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(example_text)\n",
    "print(f\"Number of tokens: {len(inputs['input_ids'])}\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll go through each component of BERT to understand how the sub-word representing token IDs are transformed to a embedding that contains semantic and contextual info, and further transformer to the output of a task.\n",
    "\n",
    "Hugging Face's BERT implementation is divided into 3 components:\n",
    "- \"Embedding\": performs embedding lookup on each ID feature and combines the ID features\n",
    "- \"Encoder\": performs the semantic and contextual embedding\n",
    "- \"Pooler\": takes the embedding at the CLS token position and applies a single feed forward layer.\n",
    "\n",
    "<br>\n",
    "\n",
    "A note on the Pooler and Task Heads:\n",
    "\n",
    "When using a transformer for a specific task, a \"*task head*\" will be attached, which is simply an output layer that matches the outputs needed for the task, and possibly a few trainable layers between the ransformers pre-trained embedding layers and the output layer.\n",
    "\n",
    "The user can choose to feed the pooler's output to the task head, but most implementations I've seen on Hugging Face simply delete it and add their own task head that directly takes the CLS token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT's embedding layer stores three embedding matrices that were trained along with the rest of the model during pre-training. It first performs embedding lookup to transform the three ID features from sequences of scalars to sequences of multidimensional vectors.\n",
    "\n",
    "The sum of the three lookup embedding vectors is then layer normalized for each element of the input sequence to create a single input token embedding for each element the input sequence (see [source code](https://github.com/huggingface/transformers/blob/0b5c7e4838fddeb4601acbb80cd2c71ddcf76726/src/transformers/models/bert/modeling_bert.py#L234) if interested).\n",
    "\n",
    "**Layer Normalization** means normalization applied to each unit of a layer. For attention layers, this means that a mean and variance are calculated for each element in the sequence, across all features of that element. In the context of input token embeddings, each token embedding will be normalized across its embedding dimensions.\n",
    "\n",
    "Note that PyTorch's `LayerNormalization` implementation applies, by default, a learnable affine transformation after normalization, meaning you must set the seed of the random generator to the same value if you want to train the normalization layer multiple times and return the same value for the same dataset.\n",
    "\n",
    "**Dropout** is a regularization technique that involves setting the activations of a given percentage of elements in a layer to zero, resulting in a thinned neural network. The [original paper](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) suggests reselecting the thinned network for every training instance, but [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) implementations both perform it once per batch / step. In the context of input token embeddings, each element of the embedding vector of each sequence has an independent chance of being dropped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT's \"encoder\" component contains all of its 12 transformer layers. Note that a 24 layer version also exists.\n",
    "\n",
    "Each layer performs bidirectional self-attention with 12 heads, and then 3 feed forward layers sandwiched with normalization layers, skip connections, and a GELU activation function. The bottom of [this post](https://gmihaila.medium.com/%EF%B8%8F-bert-inner-workings-1c3054cd1591) has a great diagram of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets go over what outputs we can get from a Hugging Face model:\n",
    "- `attentions`: contain the attention matrices for each head of each self-attention layer of the model. One attention matrix has shape `[sequence_length, sequence_length]` and shows, for each query, the distance between that query and every key, in embedding space. This model has 12 self-attention layers, each with 12 heads, so `12*12` attention matrices.\n",
    "- `hidden_states`: the activations (i.e., final output embeddings) of the embedding layer and the 12 `BertLayer`, so 13 matrices with shape `[sequence_length, model dimensions]`\n",
    "- `last_hidden_state`: the activations of the last attention layer. Note [post processing](https://huggingface.co/docs/transformers/en/main_classes/output#model-outputs) functions may cause this value to be unequal to `hidden_states[-1]`.\n",
    "- `pooler_output`: the activations of the feed forward layer that transformer's the CLS token's final embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Attention matrices: {len(outputs.attentions)} layers of {outputs.attentions[0].shape[1]} \"\n",
    "    + f\"heads. Each matrix has shape {list(outputs.attentions[0].shape[2:])} meaning \"\n",
    "    + \"[sequence length of queries, sequence length of keys]\")\n",
    "print(\n",
    "    f\"Hidden states: {len(outputs.hidden_states)} sets (for the embedding layer + 12 attention \"\n",
    "    + f\"layers) of shape {list(outputs.hidden_states[0].shape[1:])} meaning \"\n",
    "    + \"[sequence length, model dimensions]\")\n",
    "print(\n",
    "    f\"Last hidden state: has shape {list(outputs.last_hidden_state[0].shape)} meaning \"\n",
    "    + \"[sequence length, model dimensions]\")\n",
    "print(\n",
    "    f\"Pooler output: a 1D vector of {outputs.pooler_output[0].shape[0]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand inputs, layers, and outputs of a transformers model, it would be interesting to visualise where in embedding space the input vectors start and where they end up after the transformers applies the contextual and semantic mapping that it learnt during pre-training. \n",
    "\n",
    "We'll use PCA and t-SNE to reduce the dimensionality to two for plotting. t-SNE can create reductions with different non-linear relationships, so we'll make a few plots, sampling different numbers of components, as well as PCA-only plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(tsne_components: List[int]):\n",
    "\n",
    "    # Feature IDs (scalar)\n",
    "    word_ids = token_ids.input_ids[0]\n",
    "    sentence_ids = token_ids.token_type_ids[0]\n",
    "    position_ids = list(range(len(word_ids)))\n",
    "\n",
    "    # Embedding Lookup Matrices\n",
    "    word_embedding_matrix = model.embeddings.word_embeddings.weight.detach().numpy()\n",
    "    sentence_embedding_matrix = model.embeddings.token_type_embeddings.weight.detach().numpy()\n",
    "    position_embedding_matrix = model.embeddings.position_embeddings.weight.detach().numpy()\n",
    "\n",
    "    # Input Embeddings\n",
    "    word_embeddings = word_embedding_matrix[word_ids]\n",
    "    sentence_embeddings = sentence_embedding_matrix[sentence_ids]\n",
    "    position_embeddings = position_embedding_matrix[position_ids]\n",
    "    input_token_embeddings = outputs.hidden_states[0][0].detach().numpy()\n",
    "\n",
    "    # Meaningful Output Embeddings\n",
    "    output_token_embeddings = outputs.last_hidden_state[0].detach().numpy()\n",
    "\n",
    "    # Combine all lists\n",
    "    data = np.concatenate((\n",
    "        word_embeddings, sentence_embeddings, position_embeddings,\\\n",
    "        input_token_embeddings, output_token_embeddings), axis=0)\n",
    "\n",
    "    # Define the lengths of the embeddings\n",
    "    lengths = [\n",
    "        len(word_embeddings), len(sentence_embeddings), len(position_embeddings), \\\n",
    "        len(input_token_embeddings), len(output_token_embeddings)]\n",
    "\n",
    "    # Define the plot size\n",
    "    size = 10\n",
    "    alpha = 0.3\n",
    "\n",
    "    # Create subplots, one for PCA and other for each number of components in t-SNE\n",
    "    tsne_components.insert(0, None) # plus one for PCA\n",
    "    num_rows = len(tsne_components) // 2 + len(tsne_components) % 2\n",
    "    fig, axs = plt.subplots(num_rows, 2, figsize=(15, 5 * num_rows))\n",
    "\n",
    "    # Create t-SNE plots\n",
    "    for i, n_components in enumerate(tsne_components):\n",
    "\n",
    "        # Reduce dimensionality to 2D for plotting\n",
    "        if i == 0:\n",
    "            # Use PCA to reduce dimensionality to 2D\n",
    "            pca = PCA(n_components=2)\n",
    "            plot_data = pca.fit_transform(data)\n",
    "\n",
    "            title = f'PCA to 2D'\n",
    "        else:\n",
    "            # Use PCA to reduce dimensionality to size that t-SNE can handle\n",
    "            pca = PCA(n_components=n_components)\n",
    "            data_pca = pca.fit_transform(data)\n",
    "\n",
    "            # Use t-SNE to reduce dimensionality to 2D for plotting\n",
    "            tsne = TSNE(n_components=2)\n",
    "            plot_data = tsne.fit_transform(data_pca)\n",
    "\n",
    "            title = f'PCA to {n_components}, t-SNE to 2D'\n",
    "\n",
    "        # Split the transformed data back into separate arrays\n",
    "        plot_word, plot_sentence, plot_position, plot_input_token, plot_output_token \\\n",
    "            = np.split(plot_data, np.cumsum(lengths)[:-1])\n",
    "\n",
    "        # Sentence embedding is the same for all elements of the sequence\n",
    "        plot_sentence = np.mean(plot_sentence, axis=0).reshape(1, -1)\n",
    "\n",
    "        ax = axs[i//2, i%2]\n",
    "        ax.scatter(plot_word[:, 0], plot_word[:, 1], c='r', label='Word Embeddings', s=size, alpha=alpha)\n",
    "        ax.scatter(plot_sentence[:, 0], plot_sentence[:, 1], c='g', label='Sentence Embeddings', s=size*6, alpha=alpha)\n",
    "        ax.scatter(plot_position[:, 0], plot_position[:, 1], c='b', label='Position Embeddings', s=size, alpha=alpha)\n",
    "        ax.scatter(plot_input_token[:, 0], plot_input_token[:, 1], c='y', label='Input Token Embeddings', s=size, alpha=alpha)\n",
    "        ax.scatter(plot_output_token[:, 0], plot_output_token[:, 1], c='m', label='Output Token Embeddings', s=size, alpha=alpha)\n",
    "\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings([25, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the weights of lookup matrices of the three input embedding features were trained along with the transformer layers, so the positions we see were optimized along with the output embedding space.\n",
    "\n",
    "As expected, the input embedding tokens are represented in a similar area of embedding space as the three components that it's made of. Of those components, token embeddings seem to start off with the most variance in embedding space, with position embeddings in a tighter cluster. Interestingly, positional and token type embeddings do not take on their own spaces, but share the input embedding region that word embeddings use.\n",
    "\n",
    "The output embedding space however, seems to use a completely separate area of embedding space to represent the semantic and contextual information of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping Code - Imports and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from typing import Union, List, Dict, Tuple\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from io import StringIO\n",
    "import spacy\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import fasttext.util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import logging\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "import sys\n",
    "from types import ModuleType, FunctionType\n",
    "from gc import get_referents\n",
    "\n",
    "# Custom objects know their class.\n",
    "# Function objects seem to know way too much, including modules.\n",
    "# Exclude modules as well.\n",
    "BLACKLIST = type, ModuleType, FunctionType\n",
    "\n",
    "def getsize(obj):\n",
    "    \"\"\"sum size of object & members.\"\"\"\n",
    "    if isinstance(obj, BLACKLIST):\n",
    "        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))\n",
    "    seen_ids = set()\n",
    "    size = 0\n",
    "    objects = [obj]\n",
    "    while objects:\n",
    "        need_referents = []\n",
    "        for obj in objects:\n",
    "            if not isinstance(obj, BLACKLIST) and id(obj) not in seen_ids:\n",
    "                seen_ids.add(id(obj))\n",
    "                size += sys.getsizeof(obj)\n",
    "                need_referents.append(obj)\n",
    "        objects = get_referents(*need_referents)\n",
    "    return size\n",
    "\n",
    "def crash():\n",
    "    # Get the CPU usage\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    print(f'CPU usage: {cpu_usage}%')\n",
    "    # Get the memory usage\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    print(f'Memory usage: {memory_usage}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding and Preprocessing\n",
    "\n",
    "I initially thought that a complete list of Spanish verbs would be readily accessable from multiple dictionary websites or sources. After some searching, this does not seem to be the case, but that means we get to practice more NLP!\n",
    "\n",
    "We will instead create a list of Spanish verbs by parsing verbs from a large Spanish corpus. We'll choose a corpus that focuses on modern Spanish to analyse verbs currently in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets from Hugging Face\n",
    "\n",
    "As well as models, Hugging Face hosts a vast [database](https://huggingface.co/datasets) of labelled and unlabelled datasets that can be added to by anyone. The database can be filtered by task, language, or size.\n",
    "\n",
    "Each dataset is indexed by its *dataset ID*, and is further divided into *configurations*, which are usually split along some feature (e.g., English, Spanish, etc.), and further divided into splits (tran, test, etc.). Each dataset comes with a dataset card, which shows the type, distribution, and some examples of each feature in the dataset.\n",
    "\n",
    "<u>Accessing and Dataset</u>\n",
    "\n",
    "A dataset can be accessed via Hugging Face's python library called `datasets`, or their REST API called [Dataset Server](https://huggingface.co/docs/datasets-server/index).\n",
    "\n",
    "The REST API allows one to search and index the dataset as well as get summary statistics without downloading the data. \n",
    "\n",
    "<u>Streaming a Dataset</u>\n",
    "\n",
    "Large datasets that cannot fit into memory can be streamed via both methods, although the the python library iterator returns a single instance at a time, whereas the API allows a max of 100 instances per call.\n",
    "\n",
    "In my speed tests I found the API to be faster for downloading many instances. Note that you can specify sharding configurations that might allow the python library method to surpass the REST API's 100 instance cap.\n",
    "\n",
    "<u>Working with Datasets</u>\n",
    "\n",
    "Hugging Face has their own dataset object machinery, as well as simple methods to convert to [PyTorch](https://huggingface.co/docs/datasets/v2.16.1/use_with_pytorch#data-loading) or [TensorFlow](https://huggingface.co/docs/datasets/v2.16.1/use_with_tensorflow#data-loading) dataset objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Wikepedia's [Spanish corpus](https://huggingface.co/datasets/wikimedia/wikipedia/viewer/20231101.es), comprising all Spanish entries on Wikepedia's website.\n",
    "\n",
    "Lets get acquainted with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_dataset_summary(dataset: str, config: str, split: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints a summary of the dataset's metadata.\n",
    "    \"\"\"\n",
    "    # Get Hugging Face API token\n",
    "    with open('apis.json', 'r') as f:\n",
    "        API_TOKEN = json.load(f).get('hugging_face')\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/statistics\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"config\": config,\n",
    "        \"split\": split\n",
    "    }\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve dataset summary with status code '{response.status_code}': {response.text}\")\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve dataset summary with status code '501': {\"error\":\"Job manager crashed while running this job (missing heartbeats).\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'Job manager crashed while running this job (missing heartbeats).'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'wikimedia/wikipedia'\n",
    "config = '20231101.es'\n",
    "split = 'train'\n",
    "hf_dataset_summary(dataset, config, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the wikipedia dataset does not seem to [support](https://huggingface.co/docs/datasets-server/statistics#explore-statistics-over-split-data) this summary API operation. Below is an example of normal usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 8551\n",
      "Column 'idx' has attributes ['nan_count', 'nan_proportion', 'min', 'max', 'mean', 'median', 'std', 'histogram']\n",
      "Column 'label' has attributes ['nan_count', 'nan_proportion', 'no_label_count', 'no_label_proportion', 'n_unique', 'frequencies']\n",
      "Column 'sentence' has attributes ['nan_count', 'nan_proportion', 'min', 'max', 'mean', 'median', 'std', 'histogram']\n"
     ]
    }
   ],
   "source": [
    "dataset = 'glue'\n",
    "config = 'cola'\n",
    "split = 'train'\n",
    "summary = hf_dataset_summary(dataset, config, split)\n",
    "print(f\"Rows: {summary['num_examples']}\")\n",
    "for col in summary['statistics']:\n",
    "    print(f\"Column '{col['column_name']}' has attributes {list(col['column_statistics'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another endpoint we can use to get info on the dataset's data types and data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size when loaded into memory: 32Gb \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'description': '',\n",
       "  'citation': '',\n",
       "  'homepage': '',\n",
       "  'license': '',\n",
       "  'features': {'id': {'dtype': 'string', '_type': 'Value'},\n",
       "   'url': {'dtype': 'string', '_type': 'Value'},\n",
       "   'title': {'dtype': 'string', '_type': 'Value'},\n",
       "   'text': {'dtype': 'string', '_type': 'Value'}},\n",
       "  'builder_name': 'parquet',\n",
       "  'dataset_name': 'wikipedia',\n",
       "  'config_name': '20231101.es',\n",
       "  'version': {'version_str': '0.0.0', 'major': 0, 'minor': 0, 'patch': 0},\n",
       "  'splits': {'train': {'name': 'train',\n",
       "    'num_bytes': 32483180986,\n",
       "    'num_examples': 1841155,\n",
       "    'dataset_name': None}},\n",
       "  'download_size': 3493595869,\n",
       "  'dataset_size': 32483180986},\n",
       " 'partial': False}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hf_dataset_info(dataset: str, config: str, split: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints info of the dataset's metadata.\n",
    "    \"\"\"\n",
    "    # Get Hugging Face API token\n",
    "    with open('apis.json', 'r') as f:\n",
    "        API_TOKEN = json.load(f).get('hugging_face')\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/info\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"config\": config,\n",
    "        \"split\": split\n",
    "    }\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve dataset summary with status code '{response.status_code}': {response.text}\")\n",
    "    return response.json()\n",
    "\n",
    "dataset = 'wikimedia/wikipedia'\n",
    "config = '20231101.es'\n",
    "split = 'train'\n",
    "dataset_info = hf_dataset_info(dataset, config, split)\n",
    "\n",
    "print(f\"Dataset size when loaded into memory: {round(dataset_info['dataset_info']['dataset_size']/10**9)}Gb\", '')\n",
    "dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite large (32Gb), so we won't be able to store it locally or load it all into memory; any processing will have to be done iteratively.\n",
    "\n",
    "Of the four features, `text` is the only useful one for us. Lets see how large each text is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_hf_corpus(batch_size: int = 100, offset: int = 0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns an iterable that retrieves a list of sentences\n",
    "    from the wikipedia's spanish dataset on HuggingFace.\n",
    "        Once the end of the dataset is reached, it will return an empty list []\n",
    "        for every subsequent call.\n",
    "    \"\"\"\n",
    "    dataset = 'wikimedia/wikipedia'\n",
    "    config_name = '20231101.es'\n",
    "    split = 'train'\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "    batch_size = min(batch_size, 100) # Max batch_size is 100\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"dataset\": dataset,\n",
    "            \"config\": config_name,\n",
    "            \"split\": split,\n",
    "            \"offset\": offset,\n",
    "            \"length\": batch_size\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        results = [instance[\"row\"][\"text\"] for instance in data[\"rows\"]]\n",
    "        yield results\n",
    "\n",
    "        offset += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      1000.000000\n",
      "mean      17203.568000\n",
      "std       25786.696636\n",
      "min         110.000000\n",
      "25%        2376.750000\n",
      "50%        7436.500000\n",
      "75%       20818.000000\n",
      "max      226923.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlnUlEQVR4nO3df2xV9f3H8Vdbbi+tclsLtredBfEXiCIwkHI3Z5iU/oDgL5KJYw4NgY21S7SKjk2g6PKtMuPMSJUt2WDLrE6TiRFZpRaBMQtKI8MCaYThUOGWja4tpXK55X6+f7ieeWktveWW+2n7fCQ34Zzzuee+z3lfLi/Oj3vjjDFGAAAAFomPdQEAAADnIqAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKwzJNYF9EYoFNLRo0c1bNgwxcXFxbocAADQA8YYnTx5UllZWYqP7/4YSb8MKEePHlV2dnasywAAAL3wySef6Iorruh2TL8MKMOGDZP0xQZ6PJ6orTcYDGrz5s3Ky8uTy+WK2nrRO/TDHvTCLvTDHvQiMi0tLcrOznb+He9OvwwoHad1PB5P1ANKcnKyPB4PbzQL0A970Au70A970Ive6cnlGVwkCwAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1IgooZWVluvnmmzVs2DClp6frzjvvVH19fdiY6dOnKy4uLuzxwx/+MGzMkSNHNHv2bCUnJys9PV1Lly5Ve3v7hW8NAAAYECL6NeNt27apqKhIN998s9rb2/XTn/5UeXl52r9/vy655BJn3KJFi/TEE08408nJyc6fz549q9mzZ8vr9erdd9/VsWPH9P3vf18ul0v/93//F4VNAgAA/V1EAaWysjJsev369UpPT1dtba1uvfVWZ35ycrK8Xm+X69i8ebP279+vt99+WxkZGZo4caKefPJJPfbYYyotLVViYmIvNiO6bix9S4Gz//sp6I+fmh3DagAAGHwiCijnam5uliSlpaWFzX/xxRf1xz/+UV6vV3PmzNHy5cudoyg1NTUaP368MjIynPH5+flasmSJ9u3bp0mTJnV6nUAgoEAg4Ey3tLRIkoLBoILB4IVsQpiOdbnjTZfzcXF17Hf2f+zRC7vQD3vQi8hEsp/ijDHm/MM6C4VCuv3229XU1KQdO3Y483/zm99o1KhRysrK0t69e/XYY49p6tSp+vOf/yxJWrx4sf75z3/qrbfecp7T1tamSy65RJs2bVJhYWGn1yotLdWqVas6za+oqAg7fQQAAOzV1tam7373u2pubpbH4+l2bK+PoBQVFamuri4snEhfBJAO48ePV2ZmpmbMmKFDhw7p6quv7tVrLVu2TCUlJc50S0uLsrOzlZeXd94NjEQwGFRVVZWW745XIPS/Uzx1pflRew30XEc/Zs6cKZfLFetyBjV6YRf6YQ96EZmOMyA90auAUlxcrI0bN2r79u264ooruh2bk5MjSTp48KCuvvpqeb1evffee2FjGhoaJOkrr1txu91yu92d5rtcrj55QwRCcWHXoPCmi62+6jMiRy/sQj/sQS96JpJ9FNFtxsYYFRcX67XXXtOWLVs0evTo8z5nz549kqTMzExJks/n04cffqjjx487Y6qqquTxeDRu3LhIygEAAANUREdQioqKVFFRoddff13Dhg2T3++XJKWkpCgpKUmHDh1SRUWFZs2apeHDh2vv3r166KGHdOutt+qmm26SJOXl5WncuHG67777tHr1avn9fj3++OMqKirq8igJAAAYfCI6gvLCCy+oublZ06dPV2ZmpvP405/+JElKTEzU22+/rby8PI0dO1YPP/yw5s6dqzfeeMNZR0JCgjZu3KiEhAT5fD5973vf0/e///2w700BAACDW0RHUM53w092dra2bdt23vWMGjVKmzZtiuSlAQDAIMJv8QAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOhEFlLKyMt18880aNmyY0tPTdeedd6q+vj5szOnTp1VUVKThw4fr0ksv1dy5c9XQ0BA25siRI5o9e7aSk5OVnp6upUuXqr29/cK3BgAADAgRBZRt27apqKhIO3fuVFVVlYLBoPLy8nTq1ClnzEMPPaQ33nhDr776qrZt26ajR4/q7rvvdpafPXtWs2fP1pkzZ/Tuu+/q97//vdavX68VK1ZEb6sAAEC/NiSSwZWVlWHT69evV3p6umpra3XrrbequblZv/3tb1VRUaHbbrtNkrRu3Tpdf/312rlzp6ZNm6bNmzdr//79evvtt5WRkaGJEyfqySef1GOPPabS0lIlJiZGb+sAAEC/FFFAOVdzc7MkKS0tTZJUW1urYDCo3NxcZ8zYsWM1cuRI1dTUaNq0aaqpqdH48eOVkZHhjMnPz9eSJUu0b98+TZo0qdPrBAIBBQIBZ7qlpUWSFAwGFQwGL2QTwnSsyx1vupyPi6tjv7P/Y49e2IV+2INeRCaS/dTrgBIKhfTggw/qm9/8pm688UZJkt/vV2JiolJTU8PGZmRkyO/3O2O+HE46lncs60pZWZlWrVrVaf7mzZuVnJzc2034Sk9OCYVNb9q0KeqvgZ6rqqqKdQn4L3phF/phD3rRM21tbT0e2+uAUlRUpLq6Ou3YsaO3q+ixZcuWqaSkxJluaWlRdna28vLy5PF4ovY6wWBQVVVVWr47XoFQnDO/rjQ/aq+Bnuvox8yZM+VyuWJdzqBGL+xCP+xBLyLTcQakJ3oVUIqLi7Vx40Zt375dV1xxhTPf6/XqzJkzampqCjuK0tDQIK/X64x57733wtbXcZdPx5hzud1uud3uTvNdLlefvCECoTgFzv4voPCmi62+6jMiRy/sQj/sQS96JpJ9FNFdPMYYFRcX67XXXtOWLVs0evTosOWTJ0+Wy+VSdXW1M6++vl5HjhyRz+eTJPl8Pn344Yc6fvy4M6aqqkoej0fjxo2LpBwAADBARXQEpaioSBUVFXr99dc1bNgw55qRlJQUJSUlKSUlRQsXLlRJSYnS0tLk8Xj04x//WD6fT9OmTZMk5eXlady4cbrvvvu0evVq+f1+Pf744yoqKuryKAkAABh8IgooL7zwgiRp+vTpYfPXrVun+++/X5L0y1/+UvHx8Zo7d64CgYDy8/P1/PPPO2MTEhK0ceNGLVmyRD6fT5dccokWLFigJ5544sK2BAAADBgRBRRjzHnHDB06VOXl5SovL//KMaNGjeLOGAAA8JX4LR4AAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6wyJdQH9wZU/ebPTvI+fmh2DSgAAGBw4ggIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1Ig4o27dv15w5c5SVlaW4uDht2LAhbPn999+vuLi4sEdBQUHYmMbGRs2fP18ej0epqalauHChWltbL2hDAADAwBFxQDl16pQmTJig8vLyrxxTUFCgY8eOOY+XXnopbPn8+fO1b98+VVVVaePGjdq+fbsWL14cefUAAGBAGhLpEwoLC1VYWNjtGLfbLa/X2+WyAwcOqLKyUu+//76mTJkiSVqzZo1mzZqlZ555RllZWZGWBAAABpiIA0pPbN26Venp6brssst022236ec//7mGDx8uSaqpqVFqaqoTTiQpNzdX8fHx2rVrl+66665O6wsEAgoEAs50S0uLJCkYDCoYDEat7o51ueNNj8ei73TsY/Z17NELu9APe9CLyESyn6IeUAoKCnT33Xdr9OjROnTokH7605+qsLBQNTU1SkhIkN/vV3p6engRQ4YoLS1Nfr+/y3WWlZVp1apVneZv3rxZycnJ0d4EPTkldN4xmzZtivrromtVVVWxLgH/RS/sQj/sQS96pq2trcdjox5Q5s2b5/x5/Pjxuummm3T11Vdr69atmjFjRq/WuWzZMpWUlDjTLS0tys7OVl5enjwezwXX3CEYDKqqqkrLd8crEIrrdmxdaX7UXhdd6+jHzJkz5XK5Yl3OoEYv7EI/7EEvItNxBqQn+uQUz5ddddVVGjFihA4ePKgZM2bI6/Xq+PHjYWPa29vV2Nj4ldetuN1uud3uTvNdLlefvCECoTgFznYfUHgjXjx91WdEjl7YhX7Yg170TCT7qM+/B+XTTz/ViRMnlJmZKUny+XxqampSbW2tM2bLli0KhULKycnp63IAAEA/EPERlNbWVh08eNCZPnz4sPbs2aO0tDSlpaVp1apVmjt3rrxerw4dOqRHH31U11xzjfLzvzglcv3116ugoECLFi3S2rVrFQwGVVxcrHnz5nEHDwAAkNSLIyi7d+/WpEmTNGnSJElSSUmJJk2apBUrVighIUF79+7V7bffruuuu04LFy7U5MmT9de//jXsFM2LL76osWPHasaMGZo1a5ZuueUW/eY3v4neVgEAgH4t4iMo06dPlzFffRvuW2+9dd51pKWlqaKiItKXBgAAgwS/xQMAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA60QcULZv3645c+YoKytLcXFx2rBhQ9hyY4xWrFihzMxMJSUlKTc3Vx999FHYmMbGRs2fP18ej0epqalauHChWltbL2hDAADAwBFxQDl16pQmTJig8vLyLpevXr1av/rVr7R27Vrt2rVLl1xyifLz83X69GlnzPz587Vv3z5VVVVp48aN2r59uxYvXtz7rQAAAAPKkEifUFhYqMLCwi6XGWP03HPP6fHHH9cdd9whSfrDH/6gjIwMbdiwQfPmzdOBAwdUWVmp999/X1OmTJEkrVmzRrNmzdIzzzyjrKysC9gcAAAwEEQcULpz+PBh+f1+5ebmOvNSUlKUk5OjmpoazZs3TzU1NUpNTXXCiSTl5uYqPj5eu3bt0l133dVpvYFAQIFAwJluaWmRJAWDQQWDwajV37Eud7zp8Vj0nY59zL6OPXphF/phD3oRmUj2U1QDit/vlyRlZGSEzc/IyHCW+f1+paenhxcxZIjS0tKcMecqKyvTqlWrOs3fvHmzkpOTo1F6mCenhM47ZtOmTVF/XXStqqoq1iXgv+iFXeiHPehFz7S1tfV4bFQDSl9ZtmyZSkpKnOmWlhZlZ2crLy9PHo8naq8TDAZVVVWl5bvjFQjFdTu2rjQ/aq+LrnX0Y+bMmXK5XLEuZ1CjF3ahH/agF5HpOAPSE1ENKF6vV5LU0NCgzMxMZ35DQ4MmTpzojDl+/HjY89rb29XY2Og8/1xut1tut7vTfJfL1SdviEAoToGz3QcU3ogXT1/1GZGjF3ahH/agFz0TyT6K6vegjB49Wl6vV9XV1c68lpYW7dq1Sz6fT5Lk8/nU1NSk2tpaZ8yWLVsUCoWUk5MTzXIAAEA/FfERlNbWVh08eNCZPnz4sPbs2aO0tDSNHDlSDz74oH7+85/r2muv1ejRo7V8+XJlZWXpzjvvlCRdf/31Kigo0KJFi7R27VoFg0EVFxdr3rx53MEDAAAk9SKg7N69W9/+9red6Y5rQxYsWKD169fr0Ucf1alTp7R48WI1NTXplltuUWVlpYYOHeo858UXX1RxcbFmzJih+Ph4zZ07V7/61a+isDkAAGAgiDigTJ8+XcZ89W24cXFxeuKJJ/TEE0985Zi0tDRVVFRE+tIAAGCQ4Ld4AACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdIbEuoL+68idvhk1//NTsGFUCAMDAwxEUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArDMk1gUMFFf+5M1O8z5+anYMKgEAoP/jCAoAALAOAQUAAFgn6gGltLRUcXFxYY+xY8c6y0+fPq2ioiINHz5cl156qebOnauGhoZol2GlK3/yZqcHAADorE+OoNxwww06duyY89ixY4ez7KGHHtIbb7yhV199Vdu2bdPRo0d1991390UZAACgn+qTi2SHDBkir9fbaX5zc7N++9vfqqKiQrfddpskad26dbr++uu1c+dOTZs2rS/KAQAA/UyfBJSPPvpIWVlZGjp0qHw+n8rKyjRy5EjV1tYqGAwqNzfXGTt27FiNHDlSNTU1XxlQAoGAAoGAM93S0iJJCgaDCgaDUau7Y13ueBPV9XVwJ3Re75ifbQybrivNj8prDwQd+y+aPUbv0Au70A970IvIRLKf4owx0fnX+L/+8pe/qLW1VWPGjNGxY8e0atUqffbZZ6qrq9Mbb7yhBx54ICxsSNLUqVP17W9/W08//XSX6ywtLdWqVas6za+oqFBycnI0ywcAAH2kra1N3/3ud9Xc3CyPx9Pt2KgHlHM1NTVp1KhRevbZZ5WUlNSrgNLVEZTs7Gz9+9//Pu8GRiIYDKqqqkrLd8crEIq74PWdezTkxtK3In7OYNbRj5kzZ8rlcsW6nEGNXtiFftiDXkSmpaVFI0aM6FFA6fMvaktNTdV1112ngwcPaubMmTpz5oyampqUmprqjGloaOjympUObrdbbre703yXy9Unb4hAKE6BsxceUM6trSfr5A3eWV/1GZGjF3ahH/agFz0TyT7q8+9BaW1t1aFDh5SZmanJkyfL5XKpurraWV5fX68jR47I5/P1dSkAAKCfiPoRlEceeURz5szRqFGjdPToUa1cuVIJCQm69957lZKSooULF6qkpERpaWnyeDz68Y9/LJ/Pxx08AADAEfWA8umnn+ree+/ViRMndPnll+uWW27Rzp07dfnll0uSfvnLXyo+Pl5z585VIBBQfn6+nn/++WiXAQAA+rGoB5SXX3652+VDhw5VeXm5ysvLo/3SAABggOC3eAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOlH/qnv8z5U/eTPWJQAA0C9xBAUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHW4i6cf6MndQB8/NfsiVAIAwMXBERQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6wyJdQEId+VP3ox1CQAAxBxHUAAAgHUIKAAAwDqc4hkgzj019PFTs2NUCQAAF44jKAAAwDoEFAAAYB1O8QxQXd0NxGkfAEB/wREUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrcJsxwvCNtAAAGxBQEDFCDACgrxFQBhGCBQCgv+AaFAAAYB2OoAxiXX0dfrTWw9EZAMCF4AgKAACwDgEFAABYh1M86BNckAsAuBAEFHQrWtepAAAQCQIKLoreBB13gtHqqedfD0dnAGDg4RoUAABgHY6gYMCJ1lEWjtYAQOwQUGC9G0vfUuBsXKzLAABcRJziAQAA1uEIChCBnlzsy2kgALhwMQ0o5eXl+sUvfiG/368JEyZozZo1mjp16vmfCHzJQL0VujdhqK+vmzn3dBthDEBfiVlA+dOf/qSSkhKtXbtWOTk5eu6555Sfn6/6+nqlp6fHqiwMUBczxAzUwNSVvjqi1Nug1VdfEMiF18DFF7NrUJ599lktWrRIDzzwgMaNG6e1a9cqOTlZv/vd72JVEgAAsERMjqCcOXNGtbW1WrZsmTMvPj5eubm5qqmp6TQ+EAgoEAg4083NzZKkxsZGBYPBqNUVDAbV1tamIcF4nQ1x10isDQkZtbWF6Ec3Tpw4ETY9pP1UpzHXPPJKp3m7ls0Im84pq+52zIX83Ti3xq6c+/pdfTB1tR3nOvd5PXnt3tbTm3V31Z/erKejHydOnJDL5epyTFc9Pde574O+dL73WH/Vk15cbOfu657s54vVn5MnT0qSjDHnH2xi4LPPPjOSzLvvvhs2f+nSpWbq1Kmdxq9cudJI4sGDBw8ePHgMgMcnn3xy3qzQL+7iWbZsmUpKSpzpUCikxsZGDR8+XHFx0fufdUtLi7Kzs/XJJ5/I4/FEbb3oHfphD3phF/phD3oRGWOMTp48qaysrPOOjUlAGTFihBISEtTQ0BA2v6GhQV6vt9N4t9stt9sdNi81NbXP6vN4PLzRLEI/7EEv7EI/7EEvei4lJaVH42JykWxiYqImT56s6ur/nfMKhUKqrq6Wz+eLRUkAAMAiMTvFU1JSogULFmjKlCmaOnWqnnvuOZ06dUoPPPBArEoCAACWiFlAueeee/Svf/1LK1askN/v18SJE1VZWamMjIxYlSS3262VK1d2Op2E2KAf9qAXdqEf9qAXfSfOmJ7c6wMAAHDx8GOBAADAOgQUAABgHQIKAACwDgEFAABYh4DyJeXl5bryyis1dOhQ5eTk6L333ot1Sf1KaWmp4uLiwh5jx451lp8+fVpFRUUaPny4Lr30Us2dO7fTl/UdOXJEs2fPVnJystLT07V06VK1t7eHjdm6dau+/vWvy+1265prrtH69es71TLYerl9+3bNmTNHWVlZiouL04YNG8KWG2O0YsUKZWZmKikpSbm5ufroo4/CxjQ2Nmr+/PnyeDxKTU3VwoUL1draGjZm7969+ta3vqWhQ4cqOztbq1ev7lTLq6++qrFjx2ro0KEaP368Nm3aFHEt/d35+nH//fd3+rtSUFAQNoZ+REdZWZluvvlmDRs2TOnp6brzzjtVX18fNsamz6ae1DJoROGndQaEl19+2SQmJprf/e53Zt++fWbRokUmNTXVNDQ0xLq0fmPlypXmhhtuMMeOHXMe//rXv5zlP/zhD012draprq42u3fvNtOmTTPf+MY3nOXt7e3mxhtvNLm5ueaDDz4wmzZtMiNGjDDLli1zxvzjH/8wycnJpqSkxOzfv9+sWbPGJCQkmMrKSmfMYOzlpk2bzM9+9jPz5z//2Ugyr732Wtjyp556yqSkpJgNGzaYv//97+b22283o0ePNp9//rkzpqCgwEyYMMHs3LnT/PWvfzXXXHONuffee53lzc3NJiMjw8yfP9/U1dWZl156ySQlJZlf//rXzpi//e1vJiEhwaxevdrs37/fPP7448blcpkPP/wwolr6u/P1Y8GCBaagoCDs70pjY2PYGPoRHfn5+WbdunWmrq7O7Nmzx8yaNcuMHDnStLa2OmNs+mw6Xy2DCQHlv6ZOnWqKioqc6bNnz5qsrCxTVlYWw6r6l5UrV5oJEyZ0uaypqcm4XC7z6quvOvMOHDhgJJmamhpjzBcf6vHx8cbv9ztjXnjhBePxeEwgEDDGGPPoo4+aG264IWzd99xzj8nPz3emB3svz/0HMRQKGa/Xa37xi18485qamozb7TYvvfSSMcaY/fv3G0nm/fffd8b85S9/MXFxceazzz4zxhjz/PPPm8suu8zphTHGPPbYY2bMmDHO9He+8x0ze/bssHpycnLMD37wgx7XMtB8VUC54447vvI59KPvHD9+3Egy27ZtM8bY9dnUk1oGE07xSDpz5oxqa2uVm5vrzIuPj1dubq5qampiWFn/89FHHykrK0tXXXWV5s+fryNHjkiSamtrFQwGw/bx2LFjNXLkSGcf19TUaPz48WFf1pefn6+Wlhbt27fPGfPldXSM6VgHvezs8OHD8vv9YfskJSVFOTk5Yfs+NTVVU6ZMccbk5uYqPj5eu3btcsbceuutSkxMdMbk5+ervr5e//nPf5wx3fWnJ7UMFlu3blV6errGjBmjJUuW6MSJE84y+tF3mpubJUlpaWmS7Pps6kktgwkBRdK///1vnT17ttO32GZkZMjv98eoqv4nJydH69evV2VlpV544QUdPnxY3/rWt3Ty5En5/X4lJiZ2+pHHL+9jv9/fZQ86lnU3pqWlRZ9//jm97ELHdne3T/x+v9LT08OWDxkyRGlpaVHpz5eXn6+WwaCgoEB/+MMfVF1draefflrbtm1TYWGhzp49K4l+9JVQKKQHH3xQ3/zmN3XjjTdKklWfTT2pZTCJ2VfdY+ApLCx0/nzTTTcpJydHo0aN0iuvvKKkpKQYVgbYZd68ec6fx48fr5tuuklXX321tm7dqhkzZsSwsoGtqKhIdXV12rFjR6xLQQ9wBEXSiBEjlJCQ0OlK6YaGBnm93hhV1f+lpqbquuuu08GDB+X1enXmzBk1NTWFjfnyPvZ6vV32oGNZd2M8Ho+SkpLoZRc6tru7feL1enX8+PGw5e3t7WpsbIxKf768/Hy1DEZXXXWVRowYoYMHD0qiH32huLhYGzdu1DvvvKMrrrjCmW/TZ1NPahlMCCiSEhMTNXnyZFVXVzvzQqGQqqur5fP5YlhZ/9ba2qpDhw4pMzNTkydPlsvlCtvH9fX1OnLkiLOPfT6fPvzww7AP5qqqKnk8Ho0bN84Z8+V1dIzpWAe97Gz06NHyer1h+6SlpUW7du0K2/dNTU2qra11xmzZskWhUEg5OTnOmO3btysYDDpjqqqqNGbMGF122WXOmO7605NaBqNPP/1UJ06cUGZmpiT6EU3GGBUXF+u1117Tli1bNHr06LDlNn029aSWQSXWV+na4uWXXzZut9usX7/e7N+/3yxevNikpqaGXbWN7j388MNm69at5vDhw+Zvf/ubyc3NNSNGjDDHjx83xnxx+9zIkSPNli1bzO7du43P5zM+n895fsetfHl5eWbPnj2msrLSXH755V3eyrd06VJz4MABU15e3uWtfIOtlydPnjQffPCB+eCDD4wk8+yzz5oPPvjA/POf/zTGfHEraWpqqnn99dfN3r17zR133NHlbcaTJk0yu3btMjt27DDXXntt2G2tTU1NJiMjw9x3332mrq7OvPzyyyY5ObnTba1DhgwxzzzzjDlw4IBZuXJll7e1nq+W/q67fpw8edI88sgjpqamxhw+fNi8/fbb5utf/7q59tprzenTp5110I/oWLJkiUlJSTFbt24Nu627ra3NGWPTZ9P5ahlMCChfsmbNGjNy5EiTmJhopk6danbu3BnrkvqVe+65x2RmZprExETzta99zdxzzz3m4MGDzvLPP//c/OhHPzKXXXaZSU5ONnfddZc5duxY2Do+/vhjU1hYaJKSksyIESPMww8/bILBYNiYd955x0ycONEkJiaaq666yqxbt65TLYOtl++8846R1OmxYMECY8wXt5MuX77cZGRkGLfbbWbMmGHq6+vD1nHixAlz7733mksvvdR4PB7zwAMPmJMnT4aN+fvf/25uueUW43a7zde+9jXz1FNPdarllVdeMdddd51JTEw0N9xwg3nzzTfDlveklv6uu360tbWZvLw8c/nllxuXy2VGjRplFi1a1ClA04/o6KoPksI+N2z6bOpJLYNFnDHGXOyjNgAAAN3hGhQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArPP/TQiigw94d30AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_iterator = iter_hf_corpus()\n",
    "text_lengths = []\n",
    "\n",
    "for _ in range(10):\n",
    "    text_lengths.extend([len(text) for text in next(corpus_iterator)])\n",
    "text_lengths_series = pd.Series(text_lengths)\n",
    "\n",
    "print(text_lengths_series.describe())\n",
    "text_lengths_series.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text is quite long, likely a full article instead of a single sentence. Later on we'll want to pass the texts through transformer models, which have max input sequence sizes, so we'll have to divide the text into smaller pieces.\n",
    "\n",
    "As well, since we are interested in verbs we will need a method for differentiating verbs from other words. To do this, we can use a **Parts of Speech** (**POS**) tagging model, which labels each word with its POS: noun, verb, adjective, etc. POS models can be rule based or stochastic / machine learning based. A few good POS models for the Spanish language are listed in Resources. We will use stochastic-based model as they are better able to handle ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-Speech Tagging by SpaCy\n",
    "\n",
    "SpaCy is a NLP library designed for production use. It provides out-of-the-box models, as well as a library for training models and preparing them for productions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a function to load a SpaCy model and another to perform POS on a text input and format the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_pos_lem(model_type: str = \"medium\") -> spacy.language.Language:\n",
    "    \"\"\"\n",
    "    Loads a SpaCy model for Spanish that performs:\n",
    "    - Tokenization\n",
    "    - Parts of speech tagging\n",
    "    - Dependency parcing\n",
    "    - Lemmatization\n",
    "    and disables the named entity recognition (NER) part of the model.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"small\": \"es_core_news_sm\",\n",
    "        \"medium\": \"es_core_news_md\",\n",
    "        \"large\": \"es_core_news_lg\",\n",
    "        \"transformer\": \"es_dep_news_trf\"}\n",
    "\n",
    "    ## Check if model argument is valid\n",
    "    try:\n",
    "        model_name = models[model_type]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid model name. Choose from 'small', 'medium', 'large', or 'transformer'.\")\n",
    "\n",
    "    ## Get model and download model if not installed\n",
    "    while True:\n",
    "        try:\n",
    "            model = spacy.load(model_name, disable=[\"ner\"])\n",
    "            break\n",
    "        except OSError:\n",
    "            print(f\"Installing Spacy model '{model_name}'\")\n",
    "            spacy.cli.download(model_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def spacy_pos_tagger(\n",
    "    data: Union[List[str], str], model: spacy.language.Language = None, model_type: str = None\n",
    "    ) -> np.array:\n",
    "    \"\"\"\n",
    "    Tags a text with the Part of Speech (POS) and infinitive verb of each word.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array with the following columns:\n",
    "        - text: The word itself\n",
    "        - pos: The part of speech of the word. E.g. VERB, NOUN, ADJ, etc.\n",
    "        - lemma: The infinitive verb of the word. E.g., comer, nadar, etc.\n",
    "    \"\"\"\n",
    "    ## Get SpaCy model\n",
    "    if model is None:\n",
    "        model = get_spacy_pos_lem(model_type)\n",
    "\n",
    "    ## Clean input text\n",
    "    if not isinstance(data, list):\n",
    "        docs = [model(data)]\n",
    "    else:\n",
    "        docs = model.pipe(data) # uses SpaCy's internal batching\n",
    "\n",
    "    pos = [[token.text, token.pos_, token.lemma_] for doc in docs for token in doc]\n",
    "    return pd.DataFrame(pos, columns=[\"word\", \"pos\", \"lemma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy's [docs](https://spacy.io/models/es) state a POS accuracy of 0.95 to 0.99, depending on the model. Before we use the model to create a dataset of verbs, lets confirm the model's performance using a small pre-made list of verbs as labels. I found two lists from github users; we'll use Bret's list for now. \n",
    "\n",
    "Lets inspect our dataset of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs_bret() -> List:\n",
    "    # Send a GET request to fetch the XML content\n",
    "    url = \"https://raw.githubusercontent.com/bretttolbert/verbecc/main/verbecc/data/verbs-es.xml\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the XML content\n",
    "    xml_content = response.content\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    # Parse verbs from xml content\n",
    "    verb_elements = root.findall(\".//v\")\n",
    "    verbs = [v.find(\"i\").text for v in verb_elements]\n",
    "    return verbs\n",
    "\n",
    "\n",
    "def get_verbs_ghid() -> List:\n",
    "    \"\"\"\n",
    "    Retrieves a list of verbs and definitions from ghidinelli's fred-jehle-spanish-verbs project.\n",
    "\n",
    "    Returns:\n",
    "    - A list of verbs.\n",
    "    \"\"\"\n",
    "    verbs = []\n",
    "\n",
    "    # Fetch CSV contents\n",
    "    url = 'https://raw.githubusercontent.com/ghidinelli/fred-jehle-spanish-verbs/master/jehle_verb_database.csv'\n",
    "    response = requests.get(url)\n",
    "    file = StringIO(response.text)\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    # Parse verbs from csv contents\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        verb = row[0]\n",
    "        definition = row[1] # In case I want these\n",
    "        verbs.append(verb)\n",
    "\n",
    "    verbs = list(set(verbs)) # Remove duplicates\n",
    "    return verbs\n",
    "\n",
    "\n",
    "def premade_verb_list(source: str = 'bret') -> List:\n",
    "    \"\"\"\n",
    "    Retrieves a list of verbs from one of the premade sources.\n",
    "\n",
    "    Returns:\n",
    "    - A list of verbs.\n",
    "    \"\"\"\n",
    "    if source == 'bret':\n",
    "        return get_verbs_bret()\n",
    "    elif source == 'ghid':\n",
    "        return get_verbs_ghid()\n",
    "    else:\n",
    "        raise ValueError(\"source must be one of ['bret', 'ghid']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reimplantar',\n",
       " 'engrilletar',\n",
       " 'abocadear',\n",
       " 'subministrar',\n",
       " 'pesquirir',\n",
       " 'maquilear',\n",
       " 'neviscar',\n",
       " 'desyugar',\n",
       " 'aposentar',\n",
       " 'apachurrar']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = premade_verb_list()\n",
    "random.sample(verbs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how well SpaCy's models perform POS on the premade list. Note two weaknesses of our test dataset:\n",
    "- Only samples the infinitive conjugation tense of verbs.\n",
    "- Instances are single words instead of sentences. POS models often perform better with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>94.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>1.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word\n",
       "pos         \n",
       "VERB   94.98\n",
       "NOUN    1.98\n",
       "PROPN   1.42\n",
       "ADJ     1.35\n",
       "AUX     0.14\n",
       "ADV     0.06\n",
       "NUM     0.03\n",
       "ADP     0.01\n",
       "INTJ    0.01\n",
       "PRON    0.01\n",
       "PUNCT   0.01"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = spacy_pos_tagger(verbs, model_type='medium')\n",
    "counts = pos_df[['pos', 'word']].groupby('pos').count()\n",
    "counts = (counts * 100 /counts.sum()).round(2).sort_values('word', ascending=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0.95 accuracy is close to the stated 0.98 for the medium size model. The difference may be due to:\n",
    "- rare verbs\n",
    "- a lack of context since the verbs are stand alone words\n",
    "- possibly some errors in bret's verb list; after some manual checking I found that some of the words do not exist on SpanishDict.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOUN': ['cablear', 'rebumbar', 'substantivar', 'tarifar', 'atornillar'],\n",
       " 'PROPN': ['empelechar', 'embebecer', 'apostrofar', 'atochar', 'mampresar'],\n",
       " 'ADJ': ['acaudillar', 'urajear', 'pudelar', 'jaquear', 'drapear'],\n",
       " 'AUX': ['quintaesenciar', 'enalbar', 'estar', 'enrabar', 'haber'],\n",
       " 'ADV': ['astriñir', 'costriñir', 'gandujar', 'guañir', 'lamber', 'muñir'],\n",
       " 'NUM': ['desjarretar', 'esposar', 'reembolsar'],\n",
       " 'PUNCT': ['engurruñar'],\n",
       " 'PRON': ['gañir'],\n",
       " 'INTJ': ['lamer'],\n",
       " 'ADP': ['tractorar']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mislabelled = {\n",
    "    pos: pos_df.loc[pos_df['pos'] == pos, 'lemma'].values.tolist() \\\n",
    "    for pos in pos_df['pos'].unique() if pos !='VERB'}\n",
    "sample = {\n",
    "    tag: (words if len(words) < 10 else random.sample(words, 5)) \\\n",
    "    for tag, words in mislabelled.items()}\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that AUX is also being used for some verbs, as it contains the common verbs estar, haber, saber, and ser.\n",
    "Most of the verbs, however, seem very uncommon or simply not spanish words, so we'll move ahead with this POS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Edge Cases\n",
    "\n",
    "Some Spanish words are both verbs and have other meanings, so lets test how our model performs on these worst-case situation within sentences for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_verbs_test(model):\n",
    "    \"\"\"\n",
    "    Como / comiendno        means to eat and is also a conjunction\n",
    "    Nada / nadas / nadaba   means to swim and also means \"nothing\"\n",
    "    Pasa / pasas            means to happen and also means raisins\n",
    "\n",
    "    \"\"\"\n",
    "    # We surround the verbs in context so the model can differentiate between the verb and other meaning\n",
    "    test_phrase = '\\\n",
    "        Yo como manzanas. Siempre estoy comiendo manzanas. Ella nada los sabados \\\n",
    "        y tu nadas los domingos. Antes, ella nadaba los domingos tambien. No le \\\n",
    "        pasa nada a ella cuando come pasas'\n",
    "\n",
    "    test_words = ['como', 'nada', 'nadas', 'nadaba', 'pasa', 'pasas']\n",
    "\n",
    "    array = spacy_pos_tagger(test_phrase, model)\n",
    "    df = pd.DataFrame(array, columns=['text', 'pos', 'lemma'])\n",
    "\n",
    "    df['Success'] = df['pos'] == 'VERB'\n",
    "    df = df[df['text'].isin(test_words)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trick_verbs_test(spacy.load('es_core_news_lg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were hoping to see the model correctly label each test word as a verb, but more than half the tests failed.\n",
    "\n",
    "Transformers use Multi-Head attention layers to understand sequences in the context of the rest of the sequence, so lets try SpaCy's tranformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trick_verbs_test(spacy.load('es_dep_news_trf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer was successful on two more of the tricks, but still makes error clear to a Spanish speaker.\n",
    " \n",
    "Thankfully there are not many verbs with other meanings, so we'll stick with the medium sized model for now to save on compute cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Download and Parsing Speed Test \n",
    "Next we'll parse the Wikipedia Spanish corpus with our POS model to create list of modern-use Spanish verbs.\n",
    "\n",
    "It could be interesting to see how our embedding space analysis varies with more and less common verbs. For this we'll want to make a bag-of-words, which is simply a frequency count of each word in a corpus or document.\n",
    "\n",
    "The Wikipedia dataset is to large to fit in memory, so we'll stream the dataset in chunks, and apply the POS parsing and BOW counting in the same function to avoid iterating through the large dataset twice.\n",
    "\n",
    "As well, this corpus is quite large, so we'll speed test some methods for applying POS to the streaming iterable before trying to parse the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_corpus_func() -> datasets.IterableDataset:\n",
    "    \"\"\"\n",
    "    Create's an iterable for the HuggingFace large_spanish_corpus dataset.\n",
    "        Streaming=True allows us to iterate over the dataset without loading it all into memory.\n",
    "    \"\"\"\n",
    "    dataset = 'large_spanish_corpus'\n",
    "    config_name = 'EUBookShop'\n",
    "    split = 'train'\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        dataset, name=config_name, split=split,\n",
    "        streaming=True, trust_remote_code=True)\n",
    "    return iter(dataset)\n",
    "\n",
    "def hf_corpus_api(batch_size: int = 100, offset: int = 0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns an iterable that retrieves a list of sentences\n",
    "    from the HuggingFace large_spanish_corpus dataset.\n",
    "        Once the end of the dataset is reached, it will return an empty list []\n",
    "        for every subsequent call.\n",
    "    \"\"\"\n",
    "    dataset = 'large_spanish_corpus'\n",
    "    config_name = 'EUBookShop'\n",
    "    split = 'train'\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "    batch_size = min(batch_size, 100) # Max batch_size is 100\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"dataset\": dataset,\n",
    "            \"config\": config_name,\n",
    "            \"split\": split,\n",
    "            \"offset\": offset,\n",
    "            \"length\": batch_size\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        results = [instance[\"row\"][\"text\"] for instance in data[\"rows\"]]\n",
    "        yield results\n",
    "\n",
    "        offset += batch_size\n",
    "\n",
    "\n",
    "def pos_speed_test(option: int, iterator: str, batch_size: int = 100, test_instances: int = 100):\n",
    "    \"\"\"\n",
    "    The test consists of two download methods and three POS processing methods.\n",
    "\n",
    "    Download methods:\n",
    "    - HuggingFace REST api\n",
    "    - HuggingFace python library\n",
    "\n",
    "    POS processing methods:\n",
    "    - Spacy's internal batching via .pipe()\n",
    "    - Combining instances into a single string; one sentence per instance.\n",
    "    - Simply processes each instance individually\n",
    "    \"\"\"\n",
    "    pos_model = get_spacy_pos_lem(\"medium\")\n",
    "\n",
    "    if iterator not in ['api', 'func']:\n",
    "        raise ValueError('Invalid iterator argument')\n",
    "\n",
    "    batch = []\n",
    "    results = []\n",
    "\n",
    "    instances_processed = 0\n",
    "\n",
    "    # Test process: using spacy's internal batching via .pipe()\n",
    "    if option == 1:\n",
    "        while True:\n",
    "\n",
    "            # Test download: HuggingFace REST api\n",
    "            if iterator == 'api':\n",
    "                corpus_iterator = hf_corpus_api(batch_size)\n",
    "                batch = next(corpus_iterator)\n",
    "\n",
    "            # Test download: HuggingFace python library\n",
    "            elif iterator == 'func':\n",
    "                corpus_iterator = hf_corpus_func()\n",
    "                for _ in range(batch_size):\n",
    "                    batch.append(next(corpus_iterator)['text'])\n",
    "\n",
    "            docs = pos_model.pipe(batch)\n",
    "            results.extend([[token.text, token.pos_, token.lemma_] for doc in docs for token in doc])\n",
    "\n",
    "            # Stop when we've processed the number of test instances\n",
    "            instances_processed += batch_size\n",
    "            if instances_processed >= test_instances:\n",
    "                break\n",
    "\n",
    "    # Test process: combining instances into a single string; one sentence per instance.\n",
    "    elif option == 2:\n",
    "        while True:\n",
    "\n",
    "            # Test download: HuggingFace REST api\n",
    "            if iterator == 'api':\n",
    "                corpus_iterator = hf_corpus_api(batch_size)\n",
    "                batch = next(corpus_iterator)\n",
    "\n",
    "            # Test download: HuggingFace python library\n",
    "            elif iterator == 'func':\n",
    "                corpus_iterator = hf_corpus_func()\n",
    "                for _ in range(batch_size):\n",
    "                    batch.append(next(corpus_iterator)['text'])\n",
    "\n",
    "            batch_text = ' .'.join(batch)\n",
    "            doc = pos_model(batch_text)\n",
    "            results.extend([[token.text, token.pos_, token.lemma_] for token in doc])\n",
    "\n",
    "            # Stop when we've processed the number of test instances\n",
    "            instances_processed += batch_size\n",
    "            if instances_processed >= test_instances:\n",
    "                break\n",
    "\n",
    "    # Test process: simply processes each instance individually\n",
    "    elif option == 3:\n",
    "        while True:\n",
    "\n",
    "            # Test download: HuggingFace REST api\n",
    "            if iterator == 'api':\n",
    "                corpus_iterator = hf_corpus_api(batch_size=1)\n",
    "                text = next(corpus_iterator)[0]\n",
    "                doc = pos_model(text)\n",
    "                results.extend([[token.text, token.pos_, token.lemma_] for token in doc])\n",
    "\n",
    "                # Stop when we've processed the number of test instances\n",
    "                instances_processed += 1\n",
    "                if instances_processed >= test_instances:\n",
    "                    break\n",
    "\n",
    "            # Test download: HuggingFace python library\n",
    "            elif iterator == 'func':\n",
    "                corpus_iterator = hf_corpus_func()\n",
    "                text = next(corpus_iterator)['text']\n",
    "                doc = pos_model(text)\n",
    "                results.extend([[token.text, token.pos_, token.lemma_] for token in doc])\n",
    "\n",
    "                # Stop when we've processed the number of test instances\n",
    "                instances_processed += 1\n",
    "                if instances_processed >= test_instances:\n",
    "                    break\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9 s ± 2.39 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "2.32 s ± 223 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6ff65ccc4442a4be8995bdd72e1761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a1bf89916644e29c81f8f6042ca429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 s ± 567 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "3.34 s ± 31.7 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r 3 -n 1 pos_speed_test(1, 'api', 100)\n",
    "%timeit -r 3 -n 1 pos_speed_test(2, 'api', 100)\n",
    "# %timeit -r 3 -n 1 pos_speed_test(3, 'api', 100)\n",
    "print()\n",
    "%timeit -r 3 -n 1 pos_speed_test(1, 'func', 100)\n",
    "%timeit -r 3 -n 1 pos_speed_test(2, 'func', 100)\n",
    "# %timeit -r 3 -n 1 pos_speed_test(3, 'func', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading in batches is much faster than requesting instances one at a time via the python function.\n",
    "\n",
    "As well, streaming the instances to SpaCy's model via [language.pipe](https://spacy.io/api/language#pipe) is comparable to processing them as a single text, and comes with the benefit of keeping instances separate (which is mandatory in most NLP use cases), so we'll go with that method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Embeddings\n",
    "\n",
    "______\n",
    "Now that we have a solid list of common day Spanish verbs, we can explore their representation in embedding space of some pre-trained NLP models.\n",
    "_______\n",
    "\n",
    "One of the word embedding models we will use are transformer models, which are quite complex instruments, so I've written a section clarifying how data flows through a transformer to go from text to semantic and contextual embeddings, which you can find in the [Terminology and Background Info](##Terminology-and-Background-Info) section. \n",
    "____\n",
    "We start with a list of sentences.\n",
    "\n",
    "We want a few things:\n",
    "- List of verbs; need POS tagger\n",
    "- Ability to identify conjugated verbs with their infinitive; need lemmatizer\n",
    "- Every sentence that contains the verb or one of its conjugations\n",
    "- The start and end character position of the verb or its conjugation in the sentence.\n",
    "- The ability to note the position of a verb or its conjugation multiple times in the same sentence.\n",
    "\n",
    "The POS tagger we will use is SpaCy\n",
    "The lemmatizer we will use is SpaCY\n",
    "\n",
    "The corpus is large, so we will want to iterate through it only once. What this looks like with our function processes is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    Download Hugging Face Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_hf_corpus(batch_size: int = 100, offset: int = 0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns an iterable that retrieves a list of sentences\n",
    "    from the wikipedia's spanish dataset on HuggingFace.\n",
    "        Once the end of the dataset is reached, it will return an empty list []\n",
    "        for every subsequent call.\n",
    "    \"\"\"\n",
    "    dataset = 'wikimedia/wikipedia'\n",
    "    config_name = '20231101.es'\n",
    "    split = 'train'\n",
    "\n",
    "    base_url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "    batch_size = min(batch_size, 100) # Max batch_size is 100\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"dataset\": dataset,\n",
    "            \"config\": config_name,\n",
    "            \"split\": split,\n",
    "            \"offset\": offset,\n",
    "            \"length\": batch_size\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        results = [instance[\"row\"][\"text\"] for instance in data[\"rows\"]]\n",
    "        yield results\n",
    "\n",
    "        offset += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext(model_filename):\n",
    "    # fasttext.util.download_model('es', if_exists='ignore')\n",
    "    return fasttext.load_model(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_download_spacy(model_name: str):\n",
    "    try:\n",
    "        return spacy.load(model_name)\n",
    "    except OSError:\n",
    "        command = f\"python -m spacy download {model_name}\"\n",
    "        subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    finally:\n",
    "        return spacy.load(model_name)\n",
    "\n",
    "\n",
    "def get_spacy(model_type: str = \"medium\", task: str = 'all') -> spacy.language.Language:\n",
    "    \"\"\"\n",
    "    Loads a SpaCy model for Spanish that performs tokenization and word\n",
    "    embedding via the tok2vec layer, and disables, the other model components\n",
    "    for faster processing of word embeddings.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"small\": \"es_core_news_sm\",\n",
    "        \"medium\": \"es_core_news_md\",\n",
    "        \"large\": \"es_core_news_lg\",\n",
    "        \"transformer\": \"es_dep_news_trf\"}\n",
    "\n",
    "    disable_list = {\n",
    "        \"all\": [],\n",
    "        \"lemma\": [\"ner\"],\n",
    "        \"pos\": [\"ner\", \"attribute_ruler\", \"lemmatizer\"],\n",
    "        \"embedding\": [\"morphologizer\", \"parser\", \"senter\", \"attribute_ruler\", \"lemmatizer\", \"ner\"]\n",
    "    }\n",
    "    ## Check if function arguments are valid\n",
    "    try:\n",
    "        model_name = models[model_type]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid model name. Choose from 'small', 'medium', 'large', or 'transformer'.\")\n",
    "    try:\n",
    "        disable = disable_list[task]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid task name. Choose from 'all', 'lemma', 'pos', or 'embedding'.\")\n",
    "\n",
    "    ## Get model and download model if not installed\n",
    "    while True:\n",
    "        try:\n",
    "            model = spacy.load(model_name, disable=disable)\n",
    "            break\n",
    "        except OSError:\n",
    "            print(f\"Installing Spacy model '{model_name}'\")\n",
    "            spacy.cli.download(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Text: sentence segmentation, POS tagging, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_and_pos(sent: spacy.tokens.span.Span) -> Dict[str, List[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    For a single sentence, returns a list of infinitive verbs and the\n",
    "    inclusive-start index and exclusive-end index positions of each instance of\n",
    "    that verb's conjugations and infinitive.\n",
    "    \"\"\"\n",
    "    lemma_and_pos = defaultdict(list)\n",
    "    start_position = sent[0].idx\n",
    "    for token in sent:\n",
    "        if token.pos_ in ['VERB', 'AUX']:\n",
    "            lemma_and_pos[token.lemma_].append(\n",
    "                (token.idx - start_position, token.idx + len(token) - start_position))\n",
    "    return lemma_and_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Embeddings: FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_no_context_fasttext(\n",
    "    verbs: list[str], embeddings: Dict[str, np.ndarray], model: fasttext.FastText._FastText = None\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Gets the word embeddings for a list of words using a Fasttext model.\n",
    "\n",
    "    Parameters:\n",
    "        verbs (list):\n",
    "            A list of verbs in infinitive form.\n",
    "        embeddings (dict):\n",
    "            A dictionary where keys are verbs in infinitive form and values are lists of SpaCy embeddings\n",
    "            for each instance of the verb in the corpus.\n",
    "\n",
    "    Returns:\n",
    "        returns (dict):\n",
    "            A dictionary where keys are verbs in infinitive form and values are SpaCy embeddings.\n",
    "    \"\"\"\n",
    "    for verb in verbs:\n",
    "        embeddings[verb] = model.get_word_vector(verb)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Embeddings: SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_no_context_spacy(\n",
    "    verbs: list, embeddings: Dict[str, np.ndarray], model_type: str,\n",
    "    model: spacy.language.Language\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Gets the word embeddings for a list of words using a SpaCy model.\n",
    "        Note that this function returns the embedding representation\n",
    "        of each element in the list. The elements are meant to be words.\n",
    "        If an element is more than one word, the returned vector will be\n",
    "        for that sentence.\n",
    "\n",
    "    Parameters:\n",
    "        verbs (list):\n",
    "            A list of verbs in infinitive form.\n",
    "        embeddings (dict):\n",
    "            A dictionary where keys are verbs in infinitive form and values are lists of SpaCy embeddings\n",
    "            for each instance of the verb in the corpus.\n",
    "        model_type (str):\n",
    "            A string corresponding to the SpaCy model to load.\n",
    "            One of ['small', 'medium', 'large', 'transformer'].\n",
    "    Returns:\n",
    "        returns (dict):\n",
    "            A dictionary where keys are verbs in infinitive form and values are SpaCy embeddings.\n",
    "    \"\"\"\n",
    "    if model_type == 'transformer':\n",
    "        fetch_spacy_embedding = lambda doc: doc._.trf_data.last_hidden_layer_state.data[0]\n",
    "    else:\n",
    "        fetch_spacy_embedding = lambda doc: doc.vector\n",
    "\n",
    "    docs = model.pipe(verbs)\n",
    "    for doc in docs:\n",
    "        embeddings[doc.text] = fetch_spacy_embedding(doc)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def embeddings_with_context_spacy_cnn(\n",
    "    lemma_and_pos: Dict[str, List[Tuple[int, int]]], sent: str,\n",
    "    embeddings: Dict[str, List[int]], model: spacy.language.Language\n",
    "    ) -> Dict[str, List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Firstly, uses a SpaCy CNN model to derive token embeddings for each token in a sentence.\n",
    "    Secondly, creates word embeddings as the sum of token embeddings that make up that word,\n",
    "    and does this for each position of each verb found in the sentence (as instructed by lemma_and_pos).\n",
    "    Finally, appends the word embeddings to the embeddings dictionary for this model.\n",
    "\n",
    "    Parameters:\n",
    "    lemma_and_pos (dict):\n",
    "        A dictionary where keys are verbs in infinitive form and values are lists of word ranges as tuples,\n",
    "        where each tuple is the inclusive-start index and exclusive-end index of a target word in the sentence.\n",
    "    sent (str):\n",
    "        The sentence to process token embeddings for.\n",
    "    embeddings (dict):\n",
    "        A dictionary where keys are verbs in infinitive form and values are lists of SpaCy embeddings\n",
    "        for each instance of the verb in the corpus.\n",
    "    model (spacy.language.Language):\n",
    "        The SpaCy CNN tok2vec model to create the embeddings.\n",
    "\n",
    "    Returns (dict):\n",
    "        An updated dictionary of SpaCy embeddings, where keys are verbs in infinitive form\n",
    "        and values are lists of SpaCy embeddings for each instance of the verb in the corpus.\n",
    "    \"\"\"\n",
    "    doc = model(sent)\n",
    "    for verb, word_ranges in lemma_and_pos.items():\n",
    "        for word_range in word_ranges:\n",
    "            tokens_for_word = [token for token in doc if token.idx >= word_range[0] and token.idx + len(token) <= word_range[1]]\n",
    "            word_embedding = np.sum([token.vector for token in tokens_for_word], axis=0)\n",
    "            embeddings[verb].append(word_embedding)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def embeddings_with_context_spacy_trf(\n",
    "    lemma_and_pos: Dict[str, List[Tuple[int, int]]], sent: str,\n",
    "    embeddings: Dict[str, List[int]], model: spacy.language.Language\n",
    "    ) -> Dict[str, List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Firstly, uses a SpaCy CNN model to derive token embeddings for each token in a sentence.\n",
    "    Secondly, creates word embeddings as the sum of token embeddings that make up that word,\n",
    "    and does this for each position of each verb found in the sentence (as instructed by lemma_and_pos).\n",
    "    Finally, appends the word embeddings to the embeddings dictionary for this model.\n",
    "\n",
    "    Parameters:\n",
    "    lemma_and_pos (dict):\n",
    "        A dictionary where keys are verbs in infinitive form and values are lists of word ranges as tuples,\n",
    "        where each tuple is the inclusive-start index and exclusive-end index of a target word in the sentence.\n",
    "    sent (str):\n",
    "        The sentence to process token embeddings for.\n",
    "    embeddings (dict):\n",
    "        A dictionary where keys are verbs in infinitive form and values are lists of SpaCy embeddings\n",
    "        for each instance of the verb in the corpus.\n",
    "    model (spacy.language.Language):\n",
    "        The SpaCy transformer tok2vec model to create the embeddings.\n",
    "\n",
    "    Returns (dict):\n",
    "        An updated dictionary of SpaCy embeddings, where keys are verbs in infinitive form\n",
    "        and values are lists of SpaCy embeddings for each instance of the verb in the corpus.\n",
    "    \"\"\"\n",
    "    doc = model(sent)\n",
    "    for verb, word_ranges in lemma_and_pos.items():\n",
    "        for word_range in word_ranges:\n",
    "\n",
    "            tokens_positions_for_word = [\n",
    "                i + 1 for i, token in enumerate(doc) \\\n",
    "                if token.idx >= word_range[0] and token.idx + len(token) <= word_range[1]\n",
    "            ]\n",
    "            word_embedding = np.sum([doc._.trf_data.last_hidden_layer_state.data[i] for i in tokens_positions_for_word], axis=0)\n",
    "            embeddings[verb].append(word_embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Embeddings: Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_words_to_tokens(word_range: Tuple[int, int], offset_mapping: AutoTokenizer) -> List[int]:\n",
    "    \"\"\"\n",
    "    This function returns the positions of tokens that correspond to a word, given the word range for that word\n",
    "    and an Hugging Face offset mapping.\n",
    "\n",
    "    Parameters:\n",
    "        word_range (Tuple[int, int]):\n",
    "            The inclusive-start index and exclusive-end index of a target word in the sentence.\n",
    "        offset_mapping (List[Tuple[int, int]]):\n",
    "            A list of token index tuples. Each tuple is the start and end index of the characters\n",
    "            in a sentence that the token represents.\n",
    "\n",
    "    Returns (List[int]]):\n",
    "        A list of the positions of the tokens that correspond to the target word.\n",
    "    \"\"\"\n",
    "    start_pos, end_pos = word_range\n",
    "\n",
    "    token_positions = [\n",
    "        i for i, offset in enumerate(offset_mapping) \\\n",
    "        if offset[0] >= start_pos and offset[1] <= end_pos\n",
    "    ]\n",
    "    return token_positions\n",
    "\n",
    "\n",
    "def word_embedding_from_token_embeddings(layer: torch.Tensor, token_positions: List[int]):\n",
    "    \"\"\"\n",
    "    Given a layer of token embeddings and a list of positions of tokens that correspond to a word,\n",
    "    returns the sum of the embeddings for the tokens that correspond to the word.\n",
    "\n",
    "    Parameters:\n",
    "        layer (torch.Tensor):\n",
    "            A tensor of token embeddings for a layer in a Hugging Face model.\n",
    "        token_positions (List[int]):\n",
    "            A list of positions of tokens that correspond to a word.\n",
    "\n",
    "    Returns (torch.Tensor):\n",
    "        The word embedding for the layer.\n",
    "    \"\"\"\n",
    "    return torch.stack([layer[position] for position in token_positions]).sum(dim=0)\n",
    "\n",
    "\n",
    "def word_embedding_across_layers(\n",
    "    hidden_states: torch.Tensor, token_positions: List[int], layers_index: List[int]\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Averages the word embeddings across specified layers.\n",
    "\n",
    "    Parameters:\n",
    "        hidden_states (torch.Tensor):\n",
    "            A tensor of token embeddings for all layers in a Hugging Face model.\n",
    "        token_positions (list):\n",
    "            A list of positions of tokens that correspond to a word.\n",
    "        layers_index (list):\n",
    "            A list of index positions for the layers to include in the embedding calculation.\n",
    "    \"\"\"\n",
    "    embedding_per_layer = [\n",
    "        word_embedding_from_token_embeddings(hidden_states[layer][0], token_positions) \\\n",
    "        for layer in layers_index\n",
    "    ]\n",
    "    return torch.stack(embedding_per_layer).mean(dim=0)\n",
    "\n",
    "\n",
    "def embeddings_no_context_hf(\n",
    "    verbs: list[str], embeddings: dict, layers_index: list[int],\n",
    "    tokenizer: AutoTokenizer, model: AutoModel\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Gets the word embeddings for a list of words using a Hugging Face transformers model.\n",
    "\n",
    "    Parameters:\n",
    "        verbs (list):\n",
    "            A list of verbs in infinitive form.\n",
    "        embeddings (dict):\n",
    "            A dictionary where keys are verbs in infinitive form and values are lists of SpaCy embeddings\n",
    "            for each instance of the verb in the corpus.\n",
    "        layers_index (list):\n",
    "            A list of index positions for the layers to include in the embedding calculation.\n",
    "        tokenizer (AutoTokenizer):\n",
    "            The Hugging Face tokenizer to encode the sentence.\n",
    "        model (AutoModel):\n",
    "            The Hugging Face model to create the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        returns (dict):\n",
    "            A dictionary where keys are verbs in infinitive form and values are embeddings\n",
    "            of a Hugging Face transformer model.\n",
    "    \"\"\"\n",
    "    tokens_positions = [0]\n",
    "\n",
    "    for verb in verbs:\n",
    "        inputs = tokenizer(verb, return_tensors='pt')\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        word_embedding = word_embedding_across_layers(\n",
    "            hidden_states, tokens_positions, layers_index\n",
    "            ).detach().numpy()\n",
    "        embeddings[verb] = word_embedding\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def embeddings_with_context_hf(\n",
    "    lemma_and_pos: Dict[str, List[Tuple[int, int]]], sent: str,\n",
    "    embeddings: Dict[str, List[int]], layers_index: List[int],\n",
    "    tokenizer: AutoTokenizer, model: AutoModel, is_gpt: bool = False\n",
    "    ) -> Dict[str, List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Firstly, uses a Hugging Face model to derive token embeddings for each token in a sentence.\n",
    "    Secondly, creates word embeddings as the sum of token embeddings that make up that word,\n",
    "    and does this for each position of each verb found in the sentence (as instructed by lemma_and_pos).\n",
    "    Finally, appends the word embeddings to the embeddings dictionary for this model.\n",
    "\n",
    "    Parameters:\n",
    "    lemma_and_pos (dict):\n",
    "        A dictionary where keys are verbs in infinitive form and values are lists of word ranges as tuples,\n",
    "        where each tuple is the inclusive-start index and exclusive-end index of a target word in the sentence.\n",
    "    sent (str):\n",
    "        The sentence to process token embeddings for.\n",
    "    embeddings (dict):\n",
    "        A dictionary where keys are verbs in infinitive form and values are lists of embeddings\n",
    "        from a Hugging Face model for each instance of the verb in the corpus.\n",
    "    layers_index (list):\n",
    "        A list of index positions for the layers to include in the embedding calculation.\n",
    "    tokenizer (AutoTokenizer):\n",
    "        The Hugging Face tokenizer to encode the sentence.\n",
    "    model (AutoModel):\n",
    "        The Hugging Face model to create the embeddings.\n",
    "    is_gpt (bool):\n",
    "        GPT tokenizer attaches whitespaces to the first token after the whitespace, and this is represented\n",
    "        in token offset indexes, so the token positions must account for this.\n",
    "\n",
    "    Returns (dict):\n",
    "        An updated dictionary of embeddings, where keys are verbs in infinitive form\n",
    "        and values are lists of embeddings for each instance of the verb in the corpus.\n",
    "    \"\"\"\n",
    "    # Embedding for each token of the sentence\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        sent, return_tensors='pt', return_offsets_mapping=True,\n",
    "        max_length=512, truncation=True)\n",
    "    offset_mapping = inputs.pop('offset_mapping')[0]\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    for verb, word_ranges in lemma_and_pos.items():\n",
    "        for word_range in word_ranges:\n",
    "\n",
    "            # Simple work-around for GPT tokenizer include whitespace in its offset indexes\n",
    "            if is_gpt:\n",
    "                word_range = (max(word_range[0] - 1, 0), word_range[1])\n",
    "\n",
    "            # Mapping of each target word in the sentence to the tokens that represent it\n",
    "            tokens_positions = map_words_to_tokens(word_range, offset_mapping)\n",
    "\n",
    "            # Calculate a word embedding for a word in a sentence as the sum of its tokens, averaged across specified layers\n",
    "            word_embedding = word_embedding_across_layers(\n",
    "                hidden_states, tokens_positions, layers_index\n",
    "                ).detach().numpy()\n",
    "            embeddings[verb].append(word_embedding)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Parse Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFromCorpus:\n",
    "    \"\"\"\n",
    "    Class to download and process data from a corpus of texts.\n",
    "\n",
    "\n",
    "    Use the method `derive_data` to download and derive the word counts and word embeddings.\n",
    "    Use the method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Model Names\n",
    "        self.model_name_preprocessing_spacy = 'medium'\n",
    "        self.model_name_embeddings_fasttext = 'models/cc.es.300.bin'\n",
    "        self.model_name_embeddings_spacy_cnn = 'large'\n",
    "        self.model_name_embeddings_spacy_trf = 'transformer'\n",
    "        self.model_name_embeddings_hf_bert = 'bert-base-multilingual-cased'\n",
    "        self.model_name_embeddings_hf_gpt = 'openai-community/gpt2'\n",
    "\n",
    "        # Model Objects\n",
    "        self.model_preprocessing_spacy = None\n",
    "        self.model_embeddings_fasttext = None\n",
    "        self.model_embeddings_spacy_cnn = None\n",
    "        self.model_embeddings_spacy_trf = None\n",
    "        self.model_embeddings_hf_bert = None\n",
    "        self.model_embeddings_hf_gpt = None\n",
    "        self.tokenizer_embeddings_hf_bert = None\n",
    "        self.tokenizer_embeddings_hf_gpt = None\n",
    "\n",
    "        # Locations to save Processed Data\n",
    "        self.loc_data = 'data'\n",
    "        self.loc_word_counts = os.path.join(self.loc_data, 'word_counts.pkl')\n",
    "        self.loc_embeddings_with_context = os.path.join(self.loc_data, 'embeddings_with_context.pkl')\n",
    "        self.loc_embeddings_no_context = os.path.join(self.loc_data, 'embeddings_no_context.pkl')\n",
    "\n",
    "        # Logger\n",
    "        self.logger = self.create_logger()\n",
    "\n",
    "\n",
    "    def create_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        handler = logging.FileHandler('logs/error_log.txt')\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        return logger\n",
    "\n",
    "\n",
    "    def load_models(self):\n",
    "        self.model_preprocessing_spacy = get_spacy(self.model_name_preprocessing_spacy, task='all')\n",
    "        self.model_embeddings_fasttext = get_fasttext(self.model_name_embeddings_fasttext)\n",
    "        self.model_embeddings_spacy_cnn = get_spacy(self.model_name_embeddings_spacy_cnn, task='embedding')\n",
    "        self.model_embeddings_spacy_trf = get_spacy(self.model_name_embeddings_spacy_trf, task='embedding')\n",
    "        self.model_embeddings_hf_bert = AutoModel.from_pretrained(\n",
    "        self.model_name_embeddings_hf_bert, output_hidden_states=True)\n",
    "        self.model_embeddings_hf_gpt = AutoModel.from_pretrained(\n",
    "        self.model_name_embeddings_hf_gpt, output_hidden_states=True)\n",
    "        self.tokenizer_embeddings_hf_bert = AutoTokenizer.from_pretrained(\n",
    "            self.model_name_embeddings_hf_bert)\n",
    "        self.tokenizer_embeddings_hf_gpt = AutoTokenizer.from_pretrained(\n",
    "            self.model_name_embeddings_hf_gpt)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def update_word_counts(word_counts: Dict[str, int], lemma_and_pos: Dict[str, List]) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Updates a dictionary of word counts with the number of instances\n",
    "        of each verb or its conjugations in a sentence.\n",
    "\n",
    "        Parameters:\n",
    "            word_counts (dict):\n",
    "                A dictionary where keys are verbs in infinitive form and values are the number of instances of that verb in the corpus.\n",
    "            lemma_and_pos (dict):\n",
    "                A dictionary where keys are verbs in infinitive form and values are lists of word ranges as tuples,\n",
    "                where each tuple is the inclusive-start index and exclusive-end index of a target word in the sentence.\n",
    "\n",
    "        Returns (dict):\n",
    "                An updated dictionary of word counts.\n",
    "        \"\"\"\n",
    "        for verb, positions in lemma_and_pos.items():\n",
    "            word_counts[verb] += len(positions)\n",
    "        return word_counts\n",
    "\n",
    "\n",
    "    def update_embeddings_with_context(self,\n",
    "        embeddings: Dict[str, Dict[str, List[np.ndarray]]], text: str, lemma_and_pos: Dict[str, List]\n",
    "        ) -> Dict[str, Dict[str, List[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Updates a dictionary of word embeddings for each word in the corpus.\n",
    "        See `corpus_derive_counts_embeddings` for details of the word embedding object.\n",
    "        \"\"\"\n",
    "        embeddings['spacy_cnn'] = embeddings_with_context_spacy_cnn(\n",
    "            lemma_and_pos, text,\n",
    "            embeddings['spacy_cnn'],\n",
    "            self.model_embeddings_spacy_cnn)\n",
    "        embeddings['spacy_trf'] = embeddings_with_context_spacy_trf(\n",
    "            lemma_and_pos, text,\n",
    "            embeddings['spacy_trf'],\n",
    "            self.model_embeddings_spacy_trf)\n",
    "        embeddings['bert'] = embeddings_with_context_hf(\n",
    "            lemma_and_pos, text,\n",
    "            embeddings['bert'],\n",
    "            self.tf_embedding_layers_index,\n",
    "            self.tokenizer_embeddings_hf_bert,\n",
    "            self.model_embeddings_hf_bert)\n",
    "        embeddings['gpt2'] = embeddings_with_context_hf(\n",
    "            lemma_and_pos, text,\n",
    "            embeddings['gpt2'],\n",
    "            self.tf_embedding_layers_index,\n",
    "            self.tokenizer_embeddings_hf_gpt,\n",
    "            self.model_embeddings_hf_gpt, is_gpt=True)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def corpus_derive_counts_embeddings(self, sample_size: int):\n",
    "        \"\"\"\n",
    "        Iterates through the corpus and derives word counts and embeddings with context.\n",
    "            Updates the instance data objects for each sentence so that the data can be saved\n",
    "            if a crash occurs.\n",
    "\n",
    "        Parameters:\n",
    "            sample_size (int):\n",
    "                The number of sentences to process before stopping.\n",
    "\n",
    "        Data Objects:\n",
    "            self.word_counts:\n",
    "                A dict of word counts. Each word has a count of its instances in the corpus.\n",
    "            self.word_embeddings_with_context:\n",
    "                    A dict of word embedding models. Each model has a dict of words.\n",
    "                    Each word has a 2D vector of its word embedding for that model, for each\n",
    "                    sentence of the corpus that contains the word.\n",
    "                    Example:\n",
    "                        self.word_embeddings_with_context ={\n",
    "                            'spacy_cnn': {\n",
    "                                'word1': [\n",
    "                                    [1,2,3,4,5],\n",
    "                                    [1,2,4,4,5]],\n",
    "                                'word2': [\n",
    "                                    [6,7,8,9,10],\n",
    "                                    [7,7,8,9,9]]},\n",
    "                            'bert': {\n",
    "                                ...}}\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Corpus\n",
    "        self.batch_size = 100 # max download size for Hugging Face API is 100\n",
    "        self.corpus_iterator = iter_hf_corpus(batch_size=self.batch_size)\n",
    "\n",
    "        # Models: POS and Lemmatizer, Tokenizers, Word Embedding Models\n",
    "        if self.model_preprocessing_spacy is None:\n",
    "            self.load_models()\n",
    "\n",
    "        # Data Objects\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.word_embeddings_with_context = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        # Other\n",
    "        self.tf_embedding_layers_index = [-4, -3, -2, -1] # layers to average when calculating word embeddings\n",
    "\n",
    "        # Iterate through batches of the corpus instances\n",
    "        current_samples = 0\n",
    "        while True:\n",
    "\n",
    "            # One batch of instances\n",
    "            batch_text = next(self.corpus_iterator)\n",
    "\n",
    "            # Check for end of corpus dataset\n",
    "            if not batch_text: # empty list indicates end of dataset\n",
    "                break\n",
    "\n",
    "            # Performs POS Tagging and Lemmatization to find Verbs and their Infinitive Forms\n",
    "            docs = self.model_preprocessing_spacy.pipe(batch_text)\n",
    "            sents = [sent for doc in docs for sent in doc.sents]\n",
    "\n",
    "            for sent in sents:\n",
    "                text = sent.text\n",
    "\n",
    "                # Get Position Indexes for each Verb in the Sentence\n",
    "                lemma_and_pos = get_lemma_and_pos(sent)\n",
    "                if not lemma_and_pos:\n",
    "                    continue\n",
    "\n",
    "                # Update Word Counts\n",
    "                self.word_counts = self.update_word_counts(self.word_counts, lemma_and_pos)\n",
    "\n",
    "                # Update Word Embeddings with Context\n",
    "                self.word_embeddings_with_context = self.update_embeddings_with_context(\n",
    "                    self.word_embeddings_with_context, text, lemma_and_pos)\n",
    "\n",
    "                # Early Stop if Requested Sample Size collected\n",
    "                current_samples += 1\n",
    "                if current_samples >= sample_size:\n",
    "                    return\n",
    "        return\n",
    "\n",
    "\n",
    "    def derive_embeddings_no_context(self):\n",
    "        \"\"\"\n",
    "        Derives the word embeddings without context (single words passed to word embedding model)\n",
    "        for the current set of verbs collected.\n",
    "\n",
    "        Data Objects:\n",
    "            embeddings_no_context (dict):\n",
    "                    A dict of word embedding models. Each model has a dict of words.\n",
    "                    Each word has a 1D vector of its word embedding for that model.\n",
    "                    Example:\n",
    "                        embeddings_no_context = {\n",
    "                            'word2vec': {\n",
    "                                'word1': [1,2,3,4,5],\n",
    "                                'word2': [6,7,8,9,10]},\n",
    "                            'fasttext': {\n",
    "                                'word1': [1,2,3,4,5],\n",
    "                                'word2': [1,2,3,4,5]}}\n",
    "        \"\"\"\n",
    "        if self.model_embeddings_fasttext is None:\n",
    "            self.load_models()\n",
    "\n",
    "        verbs = list(self.word_counts.keys())\n",
    "        embeddings = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        embeddings['fasttext'] = embeddings_no_context_fasttext(\n",
    "            verbs, embeddings['fasttext'], self.model_embeddings_fasttext)\n",
    "        embeddings['spacy_cnn'] = embeddings_no_context_spacy(\n",
    "            verbs, embeddings['spacy_cnn'], self.model_name_embeddings_spacy_cnn,\n",
    "            self.model_embeddings_spacy_cnn)\n",
    "        embeddings['spacy_trf'] = embeddings_no_context_spacy(\n",
    "            verbs, embeddings['spacy_trf'], self.model_name_embeddings_spacy_trf,\n",
    "            self.model_embeddings_spacy_trf)\n",
    "        embeddings['bert'] = embeddings_no_context_hf(\n",
    "            verbs, embeddings['bert'], self.tf_embedding_layers_index,\n",
    "            self.tokenizer_embeddings_hf_bert,\n",
    "            self.model_embeddings_hf_bert,)\n",
    "        embeddings['gpt2'] = embeddings_no_context_hf(\n",
    "            verbs, embeddings['gpt2'], self.tf_embedding_layers_index,\n",
    "            self.tokenizer_embeddings_hf_gpt,\n",
    "            self.model_embeddings_hf_gpt,)\n",
    "        self.word_embeddings_no_context = embeddings\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def pickle_save(data, filename):\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(data, file)\n",
    "        return\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def pickle_load(filename):\n",
    "        with open(filename, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def postprocess_embeddings(data_object: Dict[str, Dict[str, List[np.ndarray]]]):\n",
    "        \"\"\"\n",
    "        Converts list of arrays to array.\n",
    "        \"\"\"\n",
    "        for model, model_embeddings in data_object.items():\n",
    "            for word, word_embeddings in model_embeddings.items():\n",
    "                data_object[model][word] = np.array(word_embeddings)\n",
    "        return data_object\n",
    "\n",
    "\n",
    "    def save_data(self):\n",
    "        # Cannot pickle a defaultdict\n",
    "        self.word_embeddings_with_context = dict(self.word_embeddings_with_context)\n",
    "        self.word_embeddings_no_context = dict(self.word_embeddings_no_context)\n",
    "\n",
    "        # Save the three data objects\n",
    "        for data_obj, loc in (\n",
    "            [self.word_counts, self.loc_word_counts],\n",
    "            [self.word_embeddings_with_context, self.loc_embeddings_with_context],\n",
    "            [self.word_embeddings_no_context, self.loc_embeddings_no_context]\n",
    "        ):\n",
    "            try:\n",
    "                self.pickle_save(data_obj, loc)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error when saving to {loc}\", exc_info=True)\n",
    "                print(f\"Error when saving to {loc}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    def derive_data(self, sample_size: int = float('inf')):\n",
    "        \"\"\"\n",
    "        Main function to call for deriving word counts and word embeddings from the corpus.\n",
    "            First, iterates through the corpus in a single pass and updates the word counts and\n",
    "            word embeddings with context after each sentence.\n",
    "            Then, with the verbs collected, derives the word embeddings without context.\n",
    "            Finally, saves the three data objects.\n",
    "        \"\"\"\n",
    "        # Iterate Through the Corpus and Derive Word Counts and Embeddings with Context\n",
    "        try:\n",
    "            self.corpus_derive_counts_embeddings(sample_size=sample_size)\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Exception occurred\", exc_info=True)\n",
    "            print('Stopped iterating through the corpus early.')\n",
    "        finally:\n",
    "            self.word_embeddings_with_context = self.postprocess_embeddings(\n",
    "                self.word_embeddings_with_context)\n",
    "\n",
    "        # Derive the word embeddings without context for the current set of verbs collected\n",
    "        try:\n",
    "            self.derive_embeddings_no_context()\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Exception occurred\", exc_info=True)\n",
    "            print('Error when deriving embeddings without context.')\n",
    "\n",
    "        # Save the data\n",
    "        self.save_data()\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def load_word_counts(self):\n",
    "        return self.pickle_load(self.loc_word_counts)\n",
    "\n",
    "    def load_embeddings_with_context(self):\n",
    "        return self.pickle_load(self.loc_embeddings_with_context)\n",
    "\n",
    "    def load_embeddings_no_context(self):\n",
    "        return self.pickle_load(self.loc_embeddings_no_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw 646 instances.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m DataFromCorpus()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderive_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      5\u001b[0m     corpus\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[54], line 295\u001b[0m, in \u001b[0;36mDataFromCorpus.derive_data\u001b[0;34m(self, sample_size)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Iterate Through the Corpus and Derive Word Counts and Embeddings with Context\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_derive_counts_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[54], line 189\u001b[0m, in \u001b[0;36mDataFromCorpus.corpus_derive_counts_embeddings\u001b[0;34m(self, sample_size)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_word_counts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_counts, lemma_and_pos)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Update Word Embeddings with Context\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings_with_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_embeddings_with_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings_with_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma_and_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Early Stop if Requested Sample Size collected\u001b[39;00m\n\u001b[1;32m    193\u001b[0m current_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[54], line 101\u001b[0m, in \u001b[0;36mDataFromCorpus.update_embeddings_with_context\u001b[0;34m(self, embeddings, text, lemma_and_pos)\u001b[0m\n\u001b[1;32m     93\u001b[0m embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_cnn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m embeddings_with_context_spacy_cnn(\n\u001b[1;32m     94\u001b[0m     lemma_and_pos, text,\n\u001b[1;32m     95\u001b[0m     embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_cnn\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_embeddings_spacy_cnn)\n\u001b[1;32m     97\u001b[0m embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_trf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m embeddings_with_context_spacy_trf(\n\u001b[1;32m     98\u001b[0m     lemma_and_pos, text,\n\u001b[1;32m     99\u001b[0m     embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_trf\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_embeddings_spacy_trf)\n\u001b[0;32m--> 101\u001b[0m embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43membeddings_with_context_hf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlemma_and_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_embedding_layers_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_embeddings_hf_bert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_embeddings_hf_bert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m embeddings_with_context_hf(\n\u001b[1;32m    108\u001b[0m     lemma_and_pos, text,\n\u001b[1;32m    109\u001b[0m     embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_embedding_layers_index,\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_embeddings_hf_gpt,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_embeddings_hf_gpt, is_gpt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "Cell \u001b[0;32mIn[53], line 142\u001b[0m, in \u001b[0;36membeddings_with_context_hf\u001b[0;34m(lemma_and_pos, sent, embeddings, layers_index, tokenizer, model, is_gpt)\u001b[0m\n\u001b[1;32m    138\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m    139\u001b[0m     sent, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, return_offsets_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    141\u001b[0m offset_mapping \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 142\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m verb, word_ranges \u001b[38;5;129;01min\u001b[39;00m lemma_and_pos\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = DataFromCorpus()\n",
    "try:\n",
    "    corpus.derive_data()\n",
    "except Exception as e:\n",
    "    corpus.logger.error(\"Exception occurred\", exc_info=True)\n",
    "    print(\"Failed to derive data from corpus.\")\n",
    "    try:\n",
    "        corpus.derive_data(sample_size=500000)\n",
    "    except Exception as e:\n",
    "        corpus.logger.error(\"Exception occurred\", exc_info=True)\n",
    "        print(\"Failed to derive data from corpus.\")\n",
    "        corpus.derive_data(sample_size=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how SpaCy's Spanish transformer model performs on our verb list within the context of the corpus sentences. We'll look at verbs labelled Auxiliary separately as they seemed to be a second category for verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bret_mislabelled_verbs = [word for words in bret_mislabelled.values() for word in words] # infinitives\n",
    "word_info = spacy_pos_tagger(bret_mislabelled_verbs, spacy.load('es_dep_news_trf'))\n",
    "frequency = [bow[word] for word in bret_mislabelled_verbs]\n",
    "df = pd.DataFrame({'verb': word_info[:, 0], 'pos': word_info[:, 1], 'frequency': frequency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean verb frequency: {np.mean(list(bow.values())).round(1)}\")\n",
    "mask = (df['frequency'] > 10) & (df['pos'] != 'AUX')\n",
    "df[mask].sort_values('frequency', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vocab(self, model = None) -> list:\n",
    "    \"\"\"\n",
    "    Downloads the vocabulary from the 'es_core_news_md' model.\n",
    "\n",
    "    Returns:\n",
    "    - A list of words.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = spacy.load('es_core_news_md')\n",
    "    return [word.lower() for word in model.vocab.strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1530.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.862745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>61.464699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2220.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word Count\n",
       "count  1530.000000\n",
       "mean      7.862745\n",
       "std      61.464699\n",
       "min       1.000000\n",
       "25%       1.000000\n",
       "50%       1.000000\n",
       "75%       4.000000\n",
       "max    2220.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "corpus = DataFromCorpus()\n",
    "embeddings = corpus.load_embeddings_with_context()\n",
    "embeddings_example_model = embeddings['spacy_cnn']\n",
    "\n",
    "# Get the number of times each word was seen\n",
    "\n",
    "# Assume 'word_counts' is your list of word counts\n",
    "word_counts = [len(arr) for arr in embeddings_example_model.values()]\n",
    "\n",
    "# Create a DataFrame from the word counts\n",
    "df = pd.DataFrame(word_counts, columns=['Word Count'])\n",
    "df.describe()\n",
    "\n",
    "# # Calculate the percentiles\n",
    "# percentiles = [np.percentile(word_counts, p) for p in range(10, 101, 10)]\n",
    "\n",
    "# # Calculate the counts and percentages in each percentile\n",
    "# counts = [np.sum(df['Word Count'] <= p) for p in percentiles]\n",
    "# percentages = [count / len(df) * 100 for count in counts]\n",
    "\n",
    "# # Create a DataFrame from the counts and percentages\n",
    "# percentile_df = pd.DataFrame({\n",
    "#     'Percentile': range(10, 101, 10),\n",
    "#     'Count': counts,\n",
    "#     'Percentage': percentages,\n",
    "#     'Percentiles': percentiles\n",
    "# })\n",
    "\n",
    "# print(percentile_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### WTF - SpaCy Tokenizer Analysis\n",
    "I dont understand why spacy tokenizer is linear in 2d pca and why new word embeddings add perfectly to that line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spacy.load('es_core_news_md').tokenizer('me gusta vasos, me gusto caballos, y te enoja copas')\n",
    "xx = spacy.load('es_core_news_md').tokenizer('espejo espejo en la pared, quien es el mas bello de todos?')\n",
    "xxx = spacy.load('es_core_news_md').tokenizer('el cesped es verde')\n",
    "data_x = np.array([np.append([z.text, 0], z.vector) for z in x])\n",
    "data_xx = np.array([np.append([z.text, 1], z.vector) for z in xx])\n",
    "data_xxx = np.array([np.append([z.text, 2], z.vector) for z in xxx])\n",
    "data = np.vstack((data_x, data_xx, data_xxx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dimensions_xxx.shape\n",
    "pca_dimensions.shape\n",
    "# data[:, 0:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca = pca.fit(data_xx[:, 1:])\n",
    "pca_dimensions_x = pca.transform(data_x[:, 1:])\n",
    "pca_dimensions_xx = pca.transform(data_xx[:, 1:])\n",
    "pca_dimensions_xxx = pca.transform(data_xxx[:, 1:])\n",
    "pca_dimensions = np.vstack((pca_dimensions_x, pca_dimensions_xx, pca_dimensions_xxx))\n",
    "pca_results = np.hstack((data[:, 0:2], pca_dimensions))\n",
    "\n",
    "plt.scatter(pca_results[:, 2], pca_results[:, 3], c=data[:, 1].astype(int), cmap='viridis')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar(label='Color scale')\n",
    "for i in range(len(pca_results)):\n",
    "    plt.text(pca_results[i, 2], pca_results[i, 3], pca_results[i, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'pca_results' is a numpy array and 'words' is a list of words\n",
    "for i in range(pca_results.shape[0]):\n",
    "    plt.scatter(pca_results[i, 2], pca_results[i, 3])\n",
    "    plt.text(pca_results[i, 2], pca_results[i, 3], pca_results[i, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.tokenizer('me gusta vasos, me gusto caballos, y te enoja copas')\n",
    "y = model.tokenizer('el cesped es verde')\n",
    "tokens = np.array([z for z in x])\n",
    "text = np.array([z.text for z in x])\n",
    "similarity_matrix = np.array([[token1.similarity(token2) for token2 in tokens] for token1 in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'similarity_matrix' is a numpy array\n",
    "sns.heatmap(similarity_matrix, annot=True, fmt=\".2f\", xticklabels=text, yticklabels=text)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)  # Optional: to keep y-axis labels horizontal\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming data is your 300-dimensional data\n",
    "# data = np.load('your_data.npy')\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=15)\n",
    "pca_result = pca.fit_transform(data[:, 2:])\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=3, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(pca_result)\n",
    "\n",
    "# Plot the two dimensions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "colors = scaler.fit_transform(data[:, 1].reshape(-1, 1))\n",
    "cmap = plt.cm.get_cmap('viridis')  # or any other colormap\n",
    "colors = cmap(colors)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "for i in range(len(tsne_results)):\n",
    "    plt.scatter(tsne_results[i,0], tsne_results[i,1], c=colors[i])\n",
    "    plt.text(tsne_results[i,0], tsne_results[i,1], str(data[i, 0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
